\PassOptionsToPackage{numbers,sort&compress}{natbib}
\documentclass[preprintnumbers,amsmath,amssymb,prd, notitlepage,nofootinbib, superscriptaddress]{revtex4}

\usepackage{amsfonts,amssymb,amsmath}
\usepackage{gensymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{aas_macros}
\usepackage{hyperref}
\usepackage{array}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\definecolor{lightgray}{gray}{0.95}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, decorations.pathreplacing, calc}

\newcommand{\Npix}{N_{\mathrm{pix}}}
\newcommand{\Planck}{{\it Planck}~}
\newcommand{\be}{\begin{equation}}        
\newcommand{\ee}{\end{equation}}
\newcommand{\cov}[0]{$\mathbf{R}$ }
\newcommand{\lb}{\left(}
\newcommand{\rb}{\right)}
\newcommand{\ba}[1]{\begin{align}
#1
\end{align}}

\newcommand{\helen}[1]{{\textcolor{red}{{[Helen: #1]}}}}
    
\begin{document}

\title{Simulation-free machine learning approaches to signal-preserving CMB foreground cleaning}

\author{Helen Shao}
\email{hs894@cam.ac.uk}
\date{\today}

\begin{abstract}

The Cosmic Microwave Background (CMB) serves as a fundamental probe into the early universe, yet its accurate extraction is hindered by contamination from complex foreground signals, particularly Galactic contributions. As CMB observations are made at various frequencies, one can extract information about different components using their frequency dependence. Internal Linear Combination (ILC) methods for foreground mitigation do this by leveraging linear combinations of multiple frequency channels. However, there are several drawbacks with these approaches. First, ILC optimization fails to account for non-linear or non-Gaussian information in foreground structures, leading to suboptimal performance. Second, the absence of a unified probabilistic model for the weights in the ILC method limits the ability to provide robust and systematic quantifications of uncertainties. To address the first question, this project builds upon the signal-preserving hybrid machine learning (ML) procedure introduced in \cite{mccarthy24_ml} to enhance CMB foreground cleaning and improve upon ILC reconstructions. We extend upon this by using the fact that the foreground modes exhibit scale-dependent correlations while the modes of the true CMB signal are independent across scales. Hence, by training ML models to predict foreground contamination at one scale using data from other other scales, this method retains the signal-preserving characteristic of the hybrid-ML technique while incorporating additional non-linear correlations. To address the second question, we also plan to explore a probabilistic derivation of the ILC solution in a Bayesian framework, using maximum-likelihood approaches in order to take advantage of the full posterior distribution of the optimal ILC weights.
 \end{abstract}
 
\maketitle

\section{Introduction}

The Cosmic Microwave Background (CMB) is a cornerstone of modern cosmology, providing not only a snapshot of the physics on the surface of last scattering but also of the evolution of structure growth in the intermediate- and late-time universe. Of particular interest is the polarization of the CMB, which can be decomposed into E-mode and B-mode components. The B-mode polarization is especially significant, as it encodes the imprint of primordial gravitational waves from inflation, offering a unique opportunity to probe the physics of the early universe \citep{kamionkowski2016, zaldarriaga1997}.

Detecting the B-mode signal is a formidable challenge due to contamination from astrophysical foregrounds, such as galactic dust and synchrotron radiation, which dominate the observed signal across most frequencies. Galactic dust, in particular, is highly non-Gaussian and spatially complex, making it difficult to cleanly separate from the underlying CMB \citep{planck2018}. Achieving precise foreground removal is crucial for accurate B-mode detection, as residual contamination can either mask the signal or produce false detections.

The Internal Linear Combination (ILC) method is one of the most widely used techniques for CMB component separation. It constructs a weighted linear combination of multi-frequency maps to minimize foreground contamination while preserving the CMB signal \citep{bennett2003, eriksen2004}. However, the ILC approach has significant limitations. It assumes Gaussian CMB statistics and neglects higher order correlations in the foregrounds, leading to suboptimal cleaning, particularly for foreground structures with complex scale dependence.

Machine learning (ML) techniques have recently been explored as an alternative approach to foreground cleaning.\helen{Add more references}. ML models excel at capturing complex, non-linear relationships in data, making them well-suited for addressing the limitations of traditional methods \citep{caldeira2019}. However, most ML-based methods rely heavily on simulations for training, which introduces significant risks. Simulations of galactic foregrounds are inherently uncertain, and mismatches between simulations and real data can bias the recovered CMB signal. 

To mitigate these issues, hybrid approaches have been developed. For instance, \citet{mccarthy24_ml} proposed a method that trains ML models using only frequency differences that cancel out the CMB signal, ensuring signal preservation independent of simulation accuracy. While promising, this approach is limited to the use of signal-canceling frequency differences and does not fully exploit all statistical properties of the foregrounds.

In this work, we extend the framework of \citet{mccarthy24_ml} by incorporating multi-scale information into the foreground cleaning process. The key insight is that the true CMB signal is statistically independent across scales, while foregrounds exhibit correlations between scales. By training ML models to predict and remove foregrounds at a given scale using data from other scales, we exploit these correlations in foregrounds while preserving the CMB signal. 

% Additionally, we use Gaussian Processes (GPs) with Variational Fourier Features (VFF) to model the high-dimensional nature of CMB data with built-in uncertainty quantification \citep{hensman2018}. This scalable, data-driven approach avoids reliance on simulations and achieves robust foreground cleaning.

% The remainder of this paper is organized as follows: Section~\ref{sec:methods} describes the theoretical foundations and implementation of our method. Section~\ref{sec:simulations} presents tests on simulated data and comparisons with standard ILC techniques. Section~\ref{sec:results} applies our method to existing CMB datasets, demonstrating its real-world performance. Finally, Section~\ref{sec:conclusion} discusses the implications of our findings for future B-mode polarization measurements and potential refinements of the method.

\section{Foreground Removal with Internal Linear Combination (ILC)}\label{sec:ILC}

The Internal Linear Combination (ILC) method has been widely used to mitigate foreground contamination in CMB observations by constructing an optimal linear combination of multi-frequency temperature maps to isolate the CMB signal from various foregrounds, such as Galactic dust, synchrotron radiation, and the thermal Sunyaev-Zel'dovich (tSZ) effect. Importantly, the ILC method is signal-preserving because it weights the input maps in such a way that fully recovers the true CMB signal while minimizing the foregrounds and noise. By incorporating multi-frequency data, ILC can effectively separate foregrounds that have different frequency dependencies. For instance, dust contamination peaks at higher frequencies, while synchrotron emission is more prominent at lower frequencies. The ILC method takes advantage of these spectral differences and frequency correlations to isolate the CMB from foregrounds.

Given a set of temperature maps\footnote{The ILC solution can be used for CMB maps of any signal type, i.e polarization} \( T_i(\hat{\mathbf{n}}) \) from \( N \) frequency bands, where \( i = 1, \dots, N \), the ILC method combines these maps to produce a single map \( T_{\text{ILC}}(\hat{\mathbf{n}}) \) that is the best estimate of the CMB signal \( S(\hat{\mathbf{n}}) \), while minimizing foreground contamination. The observed frequency-dependent maps are assumed to be composed of a linear combination of the desired signal and foreground components. In other words, the CMB signal is assumed to be present in all frequency maps, and the optimal combination of these maps emphasizes the frequencies where the CMB is least contaminated by foregrounds. Hence, the observed maps can be modeled as:
\begin{equation}
T_i(\hat{\mathbf{n}}) = a_i S(\hat{\mathbf{n}}) + F_i(\hat{\mathbf{n}}) + N_i(\hat{\mathbf{n}})
\end{equation}
where \(S(\hat{\mathbf{n}}) \) is the desired CMB signal at frequency \( i \), \( a_i \) represents its known frequency dependence or spectral energy distribution (SED), \( F_i(\hat{\mathbf{n}}) \)  is the foreground contamination at frequency \( i \), and \( N_i(\hat{\mathbf{n}}) \) is the noise. The goal is to find the set of optimal weights \( w_i \) for each frequency channel such that the combined map \( T_{\text{ILC}}(\hat{\mathbf{n}}) \) is a good estimate of the CMB signal while reducing the foregrounds and noise. The cleaned map obtained by the ILC solution is thus
\begin{equation}
T_{\text{ILC}}(\hat{\mathbf{n}}) = \sum_{i=1}^N w_i T_i(\hat{\mathbf{n}}),
\end{equation}
where weights \( w_i \) are determined by \textit{minimizing the total variance} of the cleaned map subject to the constraint that the weights sum to unity:
\begin{equation}
\sum_{i=1}^N w_i = 1
\label{eq:ilc_constraint}
\end{equation}
This equation is key for ensuring that the ILC solution is signal-preserving, which can be seen by expanding the ILC solution into its constituent parts:
\begin{equation}
T_{\text{ILC}}(\hat{\mathbf{n}}) = \sum_{i=1}^N w_i T_i(\hat{\mathbf{n}}) = S(\hat{\mathbf{n}}) + \sum_{i=1}^N w_i F_i(\hat{\mathbf{n}}) + w_i N_i(\hat{\mathbf{n}})
\end{equation}
Hence, we have a constrained linear optimization problem that can be solved using Lagrange multipliers. The optimal weights \( w_i \) are determined using the covariance matrix \( \mathbf{R} \) of the observed maps which captures the correlations between different frequency maps~\cite{2004ApJ...612..633E}:
\begin{equation}
    R_{ij} = \langle T_i T_j \rangle.
\end{equation}
% % Internal Linear Combination (ILC) Derivation

% Given a set of temperature maps \( T_i(\hat{\mathbf{n}}) \) from \( N \) frequency bands, where \( i = 1, \dots, N \), the ILC method combines these maps to produce a single map \( T_{\text{ILC}}(\hat{\mathbf{n}}) \), which is the best estimate of the CMB signal \( S(\hat{\mathbf{n}}) \) while minimizing foreground contamination. The observed maps are modeled as:
% \begin{equation}
%     T_i(\hat{\mathbf{n}}) = a_i S(\hat{\mathbf{n}}) + F_i(\hat{\mathbf{n}}) + N_i(\hat{\mathbf{n}}),
% \end{equation}
% where:
% \begin{itemize}
%     \item \( S(\hat{\mathbf{n}}) \) is the desired CMB signal.
%     \item \( a_i \) represents the known frequency dependence (SED) of the CMB signal.
%     \item \( F_i(\hat{\mathbf{n}}) \) is the foreground contamination.
%     \item \( N_i(\hat{\mathbf{n}}) \) is the noise.
% \end{itemize}

% The goal is to determine the weights \( w_i \) for each frequency channel such that the cleaned map:
% \begin{equation}
%     T_{\text{ILC}}(\hat{\mathbf{n}}) = \sum_{i=1}^N w_i T_i(\hat{\mathbf{n}})
% \end{equation}
% is a good estimate of the CMB signal \( S(\hat{\mathbf{n}}) \) while minimizing the contamination from foregrounds and noise.

% The weights must satisfy the constraint:
% \begin{equation}
%     \sum_{i=1}^N w_i = 1,
% \end{equation}
% to ensure the CMB signal is preserved:
% \begin{equation}
%     T_{\text{ILC}}(\hat{\mathbf{n}}) = S(\hat{\mathbf{n}}) + \sum_{i=1}^N w_i F_i(\hat{\mathbf{n}}) + \sum_{i=1}^N w_i N_i(\hat{\mathbf{n}}).
% \end{equation}

%\section*{Derivation of the ILC Weights}

To remove contamination, we minimize the variance of \( T_{\text{ILC}}(\hat{\mathbf{n}}) \), assuming zero mean for \( S(\hat{\mathbf{n}}) \), \( F_i(\hat{\mathbf{n}}) \), and \( N_i(\hat{\mathbf{n}}) \). The variance is:
\begin{equation}
    \text{Var}(T_{\text{ILC}}) = \langle T_{\text{ILC}}^2 \rangle = \sum_{i,j} w_i w_j \langle T_i T_j \rangle = \mathbf{w}^\top \mathbf{R} \mathbf{w},
\end{equation}
where \( \mathbf{w} = [w_1, w_2, \dots, w_N]^\top \) is the vector of weights.
Accounting for the SED factors, \( \mathbf{a} = [a_1, a_2, \dots, a_N]^\top \), the constraint becomes $\mathbf{w}^\top \mathbf{a} = 1$. 
% I present the derivation the constrained optimization solution using Lagrange multipliers below. First, we define the Lagrangian:
% \begin{equation}
%     \mathcal{L}(\mathbf{w}, \lambda) = \mathbf{w}^\top \mathbf{R} \mathbf{w} + \lambda (1 - \mathbf{w}^\top \mathbf{a}),
% \end{equation}
% where \( \lambda \) is the Lagrange multiplier.
% %\subsection*{Solve for \( \mathbf{w} \)}
% Taking the derivative of \( \mathcal{L} \) with respect to \( \mathbf{w} \) and setting it to zero:
% \begin{equation}
%     \frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 2 \mathbf{R} \mathbf{w} - \lambda \mathbf{a} = 0.
% \end{equation}
% Rearranging gives:
% \begin{equation}
%     \mathbf{R} \mathbf{w} = \frac{\lambda}{2} \mathbf{a}.
% \end{equation}
% Multiplying both sides by \( \mathbf{a}^\top \):
% \begin{equation}
%     \mathbf{a}^\top \mathbf{R} \mathbf{w} = \frac{\lambda}{2} \mathbf{a}^\top \mathbf{a}.
% \end{equation}
% Using the constraint \( \mathbf{a}^\top \mathbf{w} = 1 \), solve for \( \lambda \):
% \begin{equation}
%     \frac{\lambda}{2} = \frac{1}{\mathbf{a}^\top \mathbf{R}^{-1} \mathbf{a}}.
% \end{equation}
% Substitute \( \lambda \) back into the equation for \( \mathbf{w} \):
% \begin{equation}
%     \mathbf{w} = \frac{\mathbf{R}^{-1} \mathbf{a}}{\mathbf{a}^\top \mathbf{R}^{-1} \mathbf{a}}.
% \end{equation}
% %\section*{Final Expression for the ILC Weights}
Using Lagrange multipliers, the optimal ILC weights are:
\begin{equation}
    w_i = \frac{(\mathbf{R}^{-1} \mathbf{a})_i}{\mathbf{a}^\top \mathbf{R}^{-1} \mathbf{a}},
\label{eq:ilc_solution}
\end{equation}
This solution gives a reconstruction that is signal-preserving, while minimizing the variance contributed by foregrounds and noise.
% \begin{equation}
% \mathbf{w} = (\mathbf{a}^T \mathbf{\mathcal{C}}^{-1} \mathbf{a})^{-1} \mathbf{a}^T \mathbf{\mathcal{C}}^{-1}
% \end{equation}
Again, we emphasize that since the ILC solution relies on Gaussian-only information and a number of frequency channels that is smaller than the number of components in the observed CMB, it is suboptimal. Moreover, it is important to note that the signal-preserving property of ILC can be compromised when the covariance matrix $\mathbf{R}$ is estimated by the data, leading to chance correlations that bias the reconstruction. This is referred to as the ILC bias.

% if the foreground models are inaccurate or if there are correlations between the foregrounds that are not properly accounted for in the covariance matrix. This is clear from Eq.~\ref{eq:ilc_solution} which shows that the ILC solution depends on the covariance matrix estimation, \( \mathbf{C} \). If \( \mathbf{C} \) of the foregrounds or noise are poorly estimated, the ILC reconstructed map can contain an over- or under-estimation of the CMB signal, known as the ``ILC bias``. Specifically, if this matrix is not accurately estimated or if chance correlations between the foregrounds and signal exist (particularly at the small scales), the ILC method may inadvertently favor or suppress certain components~\cite{2009A&A...493..835D}. This is particularly problematic when foregrounds have  poorly understood frequency dependence or are highly correlated in frequency space, such as at low frequencies where synchrotron radiation and free-free emission can overlap. One way this bias can be be mitigated is by using large enough domains on which the ILC solution is optimized. The two domains we consider here are the spatial (real) and harmonic spaces.

\subsection{Spatial vs. Harmonic ILC}\label{sec:hilc_ilc}
The ILC can be performed in various domains, with the implementations distinguished by the definition of the covariance matrix, $\mathbf{R}$, used in Eqn.~\ref{eq:ilc_solution}. Here, we present the ILC method in real and harmonic spaces.  

In real space, the temperature in a frequency band $i$ across the sphere, $T_i(\Omega)$, is written with a discrete pixel basis $p(\hat{\mathbf{n}})$:
\[
T_i(\Omega) = \sum_{\hat{\mathbf{n}}} T_i(\hat{\mathbf{n}}) p(\hat{\mathbf{n}}).
\]
% In general, the pixelized coefficients $T_i(\hat{\mathbf{n}})$ can be derived using:
% \[
% T_i(\hat{\mathbf{n}}) = \int d\Omega \, T_i(\Omega) p(\hat{\mathbf{n}});
% \]
% In the simplest case, $p(\hat{\mathbf{n}})$ is a step function with unit value within the area defined by the pixel and zero elsewhere.

The covariance matrix, \cov, is computed in pixel space between maps of different frequency channels $(i,j)$:
\begin{equation}
R_{ij} = \left\langle \left(T_i(\hat{\mathbf{n}}) - \left\langle T_i \right\rangle \right)\left(T_j(\hat{\mathbf{n}}) - \left\langle T_j \right\rangle \right) \right\rangle,
\end{equation}
which can be written explicitly in terms of a sum over all pixels in the map:
\begin{equation}
R_{ij} = \frac{1}{N_{\mathrm{pix}}} \sum_{\hat{\mathbf{n}}} \left(T_i(\hat{\mathbf{n}}) - \left\langle T_i \right\rangle \right)\left(T_j(\hat{\mathbf{n}}) - \left\langle T_j \right\rangle \right),
\end{equation}
where $N_{\mathrm{pix}}$ is the total number of pixels and $\left\langle T_i \right\rangle$ is the mean temperature over pixels of the $i$th map:
\begin{equation}
\left\langle T_i \right\rangle = \frac{1}{N_{\mathrm{pix}}} \sum_{\hat{\mathbf{n}}} T_i(\hat{\mathbf{n}}).
\end{equation}

If the domain encompasses the entire, unmasked sphere, the ILC weights are applied uniformly across the map and depend on the frequency channel $i$. However, one can also perform ILC on subsets of the full-sky so that the ILC weights are spatially dependent. This localized weighting is useful for treating statistically anisotropic fields, such as the highly anisotropic Galactic foreground emission. Note that this method preserves the true temperature within each subset, correlations on scales larger than the subsets are not retained \citep{mccarthy_needlet}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/noisy_cmb_temperature.pdf}
    \caption{Maps of the CMB temperature with foregrounds (dust and synchrotron) simulated at frequencies 70 and 100 GHz.}
    \label{fig:temp_cmb_fg_maps}
\end{figure}
Alternatively, Harmonic ILC (HILC) performs the weight optimization for \textit{each} mode $\ell$ in harmonic space. Using the spherical harmonic functions \( Y_{\ell m}(\hat{\mathbf{n}}) \), the temperature field \( T(\hat{\mathbf{n}}) \) on the sphere can be decomposed into spherical harmonic coefficients \( a_{\ell m} \):
\begin{equation}
T(\hat{\mathbf{n}}) = \sum_{\ell m} T_{\ell m} Y_{\ell m}(\hat{\mathbf{n}}).
\end{equation}

The coefficients \( a_{\ell m} \) are computed from the pixel-domain temperature values \( T(\hat{\mathbf{n}}) \) as:
\begin{equation}
a_{\ell m} = \sum_{\hat{\mathbf{n}}} T(\hat{\mathbf{n}}) Y_{\ell m}^*(\hat{\mathbf{n}}),
\end{equation}
In this domain, \cov is calculated from bands of multipoles, $b_{\ell}$, centered on $\ell_0$ with a chosen width $\Delta \ell$:
\begin{equation}
R_{ij}(\ell_0) = \sum_{\ell = \ell_0 - \Delta \ell/2}^{\ell_0 + \Delta \ell/2} \sum_{m=-\ell}^{\ell} a_{\ell m}^i a_{\ell m}^{j*}.
\end{equation}
The HILC weights, $w_{\ell, m}^{i}$, are then computed using Eqn.~\ref{eq:ilc_solution} and applied to the harmonic coefficients \( a^i_{\ell m} \) within each multipole band:
\begin{equation}
    \hat{a}^{b_{\ell}}_{\ell m} = \sum_{i} w_{\ell, m}^{i} a^{i, b_{\ell}}_{\ell m}.
\end{equation}
The final HILC reconstructed map is the transform of these cleaned spherical harmonic coefficients.

While HILC can leverage scale-dependent behavior of foregrounds and instrumental noise, it is suboptimal for maps with large spatial anisotropy, such as that arising from Galactic foregrounds, where spatially localized methods are needed for effective cleaning.

% This method is particularly useful when foregrounds and signals have distinct characteristics at different angular scales. \helen{note the advantages of scale-dependent weight optimization: fg's dominate on large scales!} The combined map in harmonic space is:
% \begin{equation}
% T_{\text{ILC}}^{\ell} = \sum_{i=1}^N w_i^{\ell} T_i^{\ell}
% \end{equation}
% where \( T_i^{\ell} \) represents the spherical harmonic coefficients of the \( i \)-th frequency map and \( w_i^{\ell} \) are the frequency-dependent weights for each harmonic mode \( \ell \).

% HILC allows for more flexibility in handling foregrounds that vary with angular scale, improving the accuracy of foreground removal on large angular scales or at high resolution.

\subsection{ILC on CMB Temperature Maps}\label{sec:ilc_temp}
First, I present the results for the ILC reconstruction of the CMB temperature. I use two noiseless simulated observations of Galactic mm sky temperature at 70 and 100 GHz from PySM3. The foreground considered are the  Galactic dust and synchrotron models from PySM3, denoted as \texttt{[d1, s2]}, respectively. The SED of the thermal dust emission is typically modeled as a modified blackbody (MBB) spectrum at a given temperature $T$:
\begin{equation}
A_{\text{dust}}(\nu) = \alpha_{\text{dust}} \left(\frac{\nu}{\nu_0}\right)^{\beta_{\text{dust}}} B_\nu(T),
\end{equation}
where $\alpha_{\text{dust}}$ is the amplitude of the dust emission at the reference frequency $\nu_0$, $\beta_{\text{dust}}$ is the dust spectral index.
Meanwhile, the SED of the synchrotron foreground is modeled as:
\begin{equation}
A_{\text{sync}}(\nu) = \alpha_{\text{sync}} \left(\frac{\nu}{\nu_0}\right)^{\beta_{\text{sync}}},
\end{equation}
where $\alpha_{\text{sync}}$ is the amplitude of the synchrotron emission at the reference frequency $\nu_0$, $\nu$ is the frequency of observation, $\beta_{\text{sync}}$ is the spectral index of the synchrotron emission. To generate these simulations, PySM3 uses non-Gaussian templates at reference frequencies taken from the \Planck Commander dust map data and the \textit{WMAP} 27 GHz synchrotron data. We refer the reader to the PySM3 documentation for more detailed discussions of these models.

I denote the dust temperature map at the $i$th frequency channel as $F_i^{Dust}(\hat{\mathbf{n}})$ and the synchrotron temperature as $F_i^{Sync}(\hat{\mathbf{n}})$, where the SEDs are taken into account. For the true CMB, I use \texttt{CAMB} to generate a realization of CMB temperature from \Planck cosmology. Thus, the input CMB observations are:
\begin{equation}
T_i(\hat{\mathbf{n}}) = a_i S(\hat{\mathbf{n}}) + F_i^{Dust}(\hat{\mathbf{n}}) + F_i^{Sync}(\hat{\mathbf{n}})
\label{eq:cmb_input_noiseless}
\end{equation}
These maps are visualized in Fig~\ref{fig:temp_cmb_fg_maps}. Note that we are using CMB temperature units so that the CMB SED is $a_i=1$ and the foreground components are scaled by $(\frac{\partial B_{\nu}}{\partial T})^{-1}$.

After performing ILC cleaning on these maps in both the spatial and harmonic spaces, I compute the power spectra of the reconstructions and compare them to those of the input maps. The results for this are shown in Fig.~\ref{fig:ilc_hilc_temp}, where I label the observed CMB spectra as ``noisy``, the reconstructed as ``cleaned``, and the CMB temperature-only spectrum as ``true``. As it can be seen from comparing the noisy and cleaned spectra, both methods are able to reduce the overall variance of the observations while maintaining the true CMB power as the lower bound. Notably, the HILC method is able to remove more of the foregrounds contamination compared to the spatial ILC at small scales, $\ell \gtrsim 3000$. This demonstrates the advantage of scale-dependent cleaning as opposed to applying the same optimized weights across all scales (see Sec.~\ref{sec:hilc_ilc}). 

The ILC residuals, defined as:
\begin{equation}
    \Delta_{\rm{ILC}} = \hat{T}^{\rm{ILC}}(\mathbf{\hat{n}}) - S(\mathbf{\hat{n}}),
\end{equation}
quantify the remaining foregrounds and noise that were not removed by the procedure. This is computed for both the spatial and harmonic ILC reconstructions and are shown in Fig.~\ref{fig:ILC_HILC_residuals}. While the HILC weights are frequency-dependent, they are not spatially local and consequently we observe a ringing pattern in the HILC reconstruction that arises from the scales associated with the galactic plane. In contrast, the ILC method is pixel-based and solves for the optimal reconstruction as a function of position, so its solution does not exhibit this effect.

% \helen{discuss the stripe pattern in the HILC residuals, and the fact that HILC seems to be under-estimating the signal in the galactic plane, resulting in negative differences?}

Finally, I calculated the cross-spectra between the noisy and cleaned spectra, $C_{\ell}^{\rm{True X Clean}}$, to verify that the ILC/HILC methods preserve the true CMB signal. We expect to see $C_{\ell}^{\rm{True X Clean}}$ to align with $C_{\ell}^{\rm{TT}}$ because by construction of the ILC/HILC weights the reconstructed maps should contain the CMB-only signal plus any remaining foregrounds that were unremoved by the procedure. Since the residual foregrounds do not correlate with the true CMB signal, the cross-spectra should reduce to the $C_{\ell}^{\rm{TT}}$ auto-spectrum, which is what we see. This confirms that the method gives an unbiased recovery of the CMB power.

\begin{figure}
    \centering
    \includegraphics[width=1.05\linewidth]{Figures/real_ilc_hilc.pdf}
    \caption{The power spectra of the reconstructed maps given by the spatial ILC (left) and harmonic ILC (right) indicate a significantly reduced variance in the temperature auto-correlations compared to those of the noisy CMB observations taken at the two indicated frequency channels. The HILC solution also has a noticeable improvement over spatial ILC at smaller scales.}
    \label{fig:ilc_hilc_temp}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/ilc_hilc_temp_residuals.pdf}
    \caption{The residuals of the reconstructed CMB temperature maps given by the real-space ILC (left) and HILC (right), defined in Eqn.~\ref{eq:nn_output}. The ringing effect exhibited in the HILC residuals demonstrate the lack of spatial locality in the HILC weights.}
    \label{fig:ILC_HILC_residuals}
\end{figure}

% Additionally, a curvature parameter can be included for better modeling:
% \begin{equation}
% S_{\text{sync}}(\nu) = A_{\text{sync}} \left(\frac{\nu}{\nu_0}\right)^{\beta_{\text{sync}} + c_{\text{sync}} \ln\left(\frac{\nu}{\nu_0}\right)},
% \end{equation}
% where $c_{\text{sync}}$ is the curvature of the synchrotron SED.

\subsection{ILC on CMB Polarization Maps}\label{sec:ilc_hilc_polarization}
I also perform ILC reconstruction for CMB B-mode polarization maps using four frequency channels at $\{93.0, 143.0, 220.0, 280.0 \}$ GHz. The foreground components that I include in each channel are the polarized dust and synchrotron emission described in Sec.~\ref{sec:ilc_temp}, as well as polarized anomalous microwave emission (AME). The latter is incorporated using PySM3's \texttt{'a2'} model so the total foregrounds map is described via a sum of the following contributions: [\texttt{d1},\texttt{s1},\texttt{a2}]. The input CMB observations obey the model described in Eqn.~\ref{eq:cmb_input_noiseless}, with $S(\hat{\mathbf{n}})$ and $F_i(\hat{\mathbf{n}})$ now representing the respective B-mode maps. Similar to before, the correlations between the frequency SEDs are accounted for by PySM3. The polarization intensity, defined as $P = \sqrt{Q^2 + U^2}$ where $Q$ and $U$ are the Stokes parameters, is depicted for each frequency channel in Fig.~\ref{fig:fg_pol_maps_ppt}. It is evident that the foreground contaminants dominate the B-mode measurements in all channels, making foreground-mitigation crucial for extracting cosmological information from this CMB component. This can also be seen quantitatively in the power spectra of the noisy CMB versus true CMB signal in Fig.~\ref{fig:ilc_spectra_Clbb}, where the foreground contributions raises the variance by increasing orders of magnitude as frequency increases. It is also important to note that the CMB B-mode polarization signal inherently much weaker than the E-mode polarization and temperature fluctuations, as evidenced by the significantly smaller magnitude in power of the $C_{\ell}^{\rm{BB}}$ spectra. Consequently, the ILC method is limited by the low signal-to-noise ratio of B-modes relative to foregrounds. 
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/fg_pol_maps_ppt.pdf}
    \caption{\centering Polarization maps of dust, synchrotron, and AME foregrounds taken at four frequency channels: 93, 143, 220, and 280 GHz.}
    \label{fig:fg_pol_maps_ppt}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/real_ilc_clbb.pdf}
    \caption{B-mode auto-correlation spectra for the noisy CMB at each of the frequency channels compared with that for the ILC reconstruction. The cross-spectra of the true CMB and ILC cleaned map also demonstrates an unbiased result.}
    \label{fig:ilc_spectra_Clbb}
\end{figure}

The power spectrum of the reconstructed map given by the spatial ILC solution is shown in Fig.~\ref{fig:ilc_spectra_Clbb}. Similar to the temperature maps, the cleaned B-mode map is also unbiased due to the alignment of the $C_{\ell}^{\rm{True X Clean}}$ and $C_{\ell}^{\rm{BB}}$. However, in this case, when comparing the spatial ILC results to the HILC method, I find that the reconstructed spectra do not noticeably differ as they did with the temperature maps. Moreover, in contrast with the temperature component, foreground removal in the B-modes is more difficult on the largest scales rather than the smaller ones, as seen in the residual variance on large-$\ell$'s in Fig.~\ref{fig:hilc_ilc_spectra}. The similar performances of the ILC and HILC can also be seen in Fig.~\ref{fig:HILC_ILC_residuals_Bmodes}, where the two reconstructions contain similar levels of residual foregrounds. 

In conclusion, while ILC methods in both harmonic and spatial domains provide effective tools for foreground cleaning, their limitations become apparent when applied to the weak and elusive B-mode polarization signal. The challenge lies in the fundamentally low signal-to-noise ratio of B-modes, coupled with the complex and spatially varying nature of foregrounds such as galactic dust and synchrotron radiation. Despite their statistical robustness and simple linear solution, ILC approaches may not be sufficient for recovering the CMB polarization signal at precisions necessary for upcoming next generation cosmological analyses. This inherent trade-off highlights the need for more adaptive methods that can take advantage of the highly non-Gaussian information present in the foregrounds. ML, with its ability to model complex and nonlinear relationships, offers a promising alternative for this problem. In the following sections, we show that ML-based approaches not only offer more flexible modeling of the foregrounds but can also be tailored to meet the requirement of signal-preservation for foreground cleaning procedures.

\begin{figure}
\label{fig:hilc_ilc_spectra}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/both_ilc_hilc_clbb.pdf}
    \caption{The auto-spectra of the real-space ILC and HILC cleaned CMB B-modes indicate the two methods provide similar foreground removal for this component.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/HILC_ILC_residuals_Bmodes.png}
    \caption{The HILC and ILC residuals for the CMB B-modes.}
    \label{fig:HILC_ILC_residuals_Bmodes}
\end{figure}



% \section{Probabilistic ILC}
% It would be interesting to develop a probabilistic formulation for the ILC solution to perform uncertainty quantification and propagation of the ILC errors to cosmological inference. Recall that the ILC method extracts the CMB signal by finding the optimal set of linear weights that combine multiple frequency maps while minimizing the variance of foregrounds and noise. One way to interpret this probabilistically is to assume that the observed data, including the CMB signal, foregrounds, and noise, is a random variable. The SEDs weigh these components based on frequency and are given by underlying physical theories. Since the foreground components are correlated among different frequency channels, they can be treated as correlated noise with an empirical covariance matrix. We absorb this component into the co-added white noise contribution and label as $\epsilon_i(\hat{n})$. Thus, we can view our observations as realizations of a forward model whose parameters can be described using probability distributions and can be inferred using maximum likelihood techniques.
% \begin{equation}
%     T_i(\hat{n}) = \Phi_i W(\hat{n}) + \epsilon_i(\hat{n})
% \end{equation}
% where W is the true CMB signal, $\epsilon \sim \mathcal{N}(\mu, \mathcal{K})$, and $\mathcal{K}$ is the measured covariance matrix. Here, we are assuming a Gaussian random variables because we are still looking at two-point correlations. We then want to maximize the likelihood of observing the CMB signal given these forward parameters:
% \begin{equation}
%     \log\mathcal{L} = \log \mathbb{P}(T_i | \Phi_i, W, \mathcal{K}) = -\frac{1}{2} \log((2\pi)^4 \, \rm{det}(\mathcal{K})) - \frac{1}{2}(T-\Phi)^{\rm T}\mathcal{K}^{\rm -1}(T-\Phi W)
% \end{equation}
% Using standard first-order conditions, this likelihood is maximized with respect to the parameters $W$:
% \begin{align}
%     0 &= \frac{1}{2}(- \phi^T \mathcal{K}^{-1} (T-\Phi w) - \phi^T \mathcal{K}^{-1} (T - \Phi W)) \\ 
%     0 &= \phi^T \mathcal{K}^{-1} (T-\Phi w)\\
%     \Phi ^T \mathcal{K}^{-1} &=  \Phi \mathcal{K}^{-1} \Phi W \\
%     \hat{W}_{\rm MLE} & = (\Phi^T  \mathcal{K}^{-1} \Phi)^{-1} \Phi^T \mathcal{K}^{-1} T
% \end{align} 

\section{Signal-independent cleaning with ML}\label{sec:ml_framework}
This section describes the ML framework discussed in \cite{mccarthy24_ml} where the authors present a method to use ML to improve the ILC method by learning and removing residuals in the ILC reconstruction. The key feature of their approach is the interpretability of unbiased, signal-preserved CMB reconstructions, and this is achieved through the use of ``frequency-difference`` information - which does not contain the true CMB component - to clean the residual contamination. 

\subsection{Framework}\label{sec:ml_framework}
Here, I follow the notation presented in \cite{mccarthy24_ml} and adapt their model of the CMB observations that are inputs to the ML network:
\begin{equation}
T_i(\hat{\mathbf{n}}) = a_i S(\hat{\mathbf{n}}) + F_i(\hat{\mathbf{n}}) + N_i(\hat{\mathbf{n}}),
\end{equation}
where $a$ is the known CMB SED, $S(\hat{\mathbf{n}})$ is the component of interest in pixel space ($p$) that one wishes to reconstruct, $i$ is the frequency channel, $F_i(\hat{\mathbf{n}})$ is the foregrounds, and $N_i(\hat{\mathbf{n}})$ is the noise. It is key that the latter two contributions are uncorrelated with $ S(\hat{\mathbf{n}})$. Hence, the goal is to train a neural network (NN) to learn the ILC residuals from only $F_i(\hat{\mathbf{n}})$ and $N_i(\hat{\mathbf{n}})$. We denote the output or target of the NN as $\tilde f(F, N)$. The ultimate reconstruction of the CMB component of interest can then be expressed with this additional contribution as:
\be
 T^{\mathrm{pred}} = \sum _i w_i T_i + \tilde f(F, N),\label{ml_pred_coi}
\ee
Since the weights $w_i$ are given by the ILC solution and are unbiased, the above can be written as:
\be
 f(T^i) = S(\hat{\mathbf{n}}) +  \sum_i w_i (F_i+N_i) + \tilde f(F, N).
 \label{eq:ultimate_reconstruction}
\ee
For the construction of the NN output, one must carefully design $\tilde f(F, N)$ such that it is independent of the true CMB signal. In general, this function is defined as a linear combination of the observed maps that nulls any contributions from $S(\hat{\mathbf{n}})$:
\be
\tilde f(F,N) \sim \tilde f\left(\left\{\sum_i c_i T_i\right\}\right).
\ee
This is achieved by requiring that $\sum_i a_ic_i=0$. %As explained in \cite{mccarthy24_ml}, any function that satisfies this condition can be used as the target of the NN and any signal-free data can be used as inputs to this function. 
Building off of the ILC method, \cite{mccarthy24_ml} choose $\tilde f$ to be the ILC residuals:
\begin{equation}
    \tilde f(F, N) = - \Delta T^{\rm{ILC}} = -( \hat{T}^{\rm{ILC}} - S(\hat{\mathbf{n}}))
    \label{eq:nn_output}
\end{equation}
This quantifies the remaining contamination that the ILC failed to remove from the CMB observations. If the NN can learn this function from data such that the improvement with the estimated $\Delta\ T^{\rm{ILC}}$ taken into account ($||\hat{\Delta T^{\rm{ILC}}}(\hat{\mathbf{n}}) - \Delta T^{\rm ILC}(\hat{\mathbf{n}})||$) is smaller in magnitude than the original ILC residual ($|| \hat T^{\rm{ILC}}-S(\hat{\mathbf{n}})||$) then the NN can contribute to the reconstruction given by the original solution in a signal-preserving way.\footnote{for a given distance measure $||\cdot||$}

They also choose the inputs of this function to be the linear combination of the foreground and noise components given by the ILC optimized weights:
\begin{equation}
    \hat{F}_i = T_i - \hat{T}^{\rm{ILC}} = (F_i+N_i) - \sum_j w_j (F_j+N_j)
    \label{eq:freq_diff}
\end{equation}
$\hat{F}_i$ can be understood as the ILC estimation of the contaminants, which relies on linear combinations of the foregrounds and noise components across different frequencies. Hence, $\hat{F}_i$ contains ``frequency-difference`` information taken from the different input channels. As discussed below, these inputs can be obtained from simulations for which we know the ``true`` CMB signal.

Taking the outputs from the NN, given by Eqn.~\ref{eq:nn_output}, we finally obtain the ultimate prediction for the reconstructed map of the desired CMB signal which I reiterate here:
\be
T^{\mathrm{pred}} = T^{\rm{ILC}} - \hat{\Delta T^{\rm ILC}}.
\ee
Since this solution is unbiased, we can write that \cite{mccarthy24_ml}: 
\be
\left< T^{\mathrm{pred}}(\hat{\mathbf{n}}) S(\hat{\mathbf{n}}) \right> = \left< S(\hat{\mathbf{n}})S(\hat{\mathbf{n}}) \right>
\ee

\subsection{Data and NN Model}

\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{6cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|}
\hline
\textbf{Metric}                & \textbf{93.0GHz} & \textbf{143.0GHz} & \textbf{220.0GHz} & \textbf{280.0GHz} \\ \hline
\textbf{Total Number of Patches} & 656                     & 656                       & 656                       & 656                       \\ \hline
\textbf{Train \& Val Input Shape} & (490, 256, 256)         & (490, 256, 256)           & (490, 256, 256)           & (490, 256, 256)           \\ \hline
\textbf{Train \& Val Patches}    & 490                     & 490                       & 490                       & 490                       \\ \hline
\textbf{Number of Train Patches} & \multicolumn{4}{c|}{[392, 4, 256, 256]} \\ \hline
\end{tabular}
\caption{Summary of patches and shapes across different frequency channels.}
\end{table}

The simulations we use to train the NN are the same ones described in Section~\ref{sec:ilc_hilc_polarization}.  There are several preprocessing steps to prepare the data for the NN. I first divide each sky simulation of the input and output datasets into quadrants in galactic coordinates (centered on our galaxy). The first three quadrants are used for training and validation, while the fourth quadrant is left for testing the NN. I then create a plate carr√©e (CAR) map for each quadrant, such each pixel corresponds to equal spacing in celestial coordinates, resulting in an equirectangular projection. Finally, I divide the CAR projection into square patches of side length $10 \degree$. Following \cite{mccarthy24_ml}, I exclude the patches at and near the poles and the equator, and then perform rotations by {90, 180, 270}$\degree$ to the training data. Realizations of these patches are visualized in Fig.~\ref{fig:car_patches}. 
\helen{I first project each sky simulation of the input and output datasets into equal-area square patches with $N_{\rm{pix}} \times N_{\rm{pix}}$, with $N_{\rm{pix}} = 256 $ pixels, with one pixel having a resolution of 0.9 arcminutes; thus our patches are $\sim$10.0 square degrees.}.
\helen{In the end, there are {X,Y,Z} patches in the training, validation, and testing sets.}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/car_patches.pdf}
    \caption{Scheme for turning the CAR projection of full-sky maps into $10\degree \times 10\degree$ square patches.}
    \label{fig:car_patches}
\end{figure}

Moreover, I follow the training procedure from  \cite{mccarthy24_ml} in which I use \texttt{camb} to generate random Gaussian realizations of \Planck 2018 $\Lambda$CDM cosmology of the CMB B-mode maps for each patch at each training epoch. When considering variations in the tensor-to-scale ratio, $r$, the values of $r$ are drawn from a log-uniform prior distribution with $-3 < log_{\rm{10}}< -1.5$. 

For tests in which I implement SO-like conditions, I convolve all maps with the beams corresponding to SO goal configurations and add the corresponding expected Gaussian white noise. For this, I use the ''goal'' configuration of Table~1 in~\cite{SO-goals}. The beam full width at half maxima are $\{2.2,1.4,1.0,0.9\}\, \mathrm{arcmin}$, and the noise levels are $\{5.8,6.3,15,37\}\, \mathrm{\mu K-arcmin}$, for the listed frequencies respectively. Lastly, I convolve all maps to a common Gaussian beam of 0.9 arcminutes. The final white noise spectrum is:
\be
N_\ell^\nu = N^\nu_{\mathrm{white}}\frac{B_\ell(\theta_{0.9\,\mathrm{arcmin}})}{B_\ell(\theta_{\mathrm{FWHM}})}
\ee
where $N^\nu_{\mathrm{white}}$ is the noise in $\mathrm{\mu K}^2$, and the Gaussian beam convolving functions $B_\ell(\theta)$ are given by 
\be
B_\ell(\theta)=\exp\lb\frac{-\lb\ell\lb\ell+1\rb\rb}{2}\frac{\theta^2}{8\ln 2}\rb,
\ee
where $\theta$ is the angle in radians. The final beam-convolved sky temperature is
\be
T^i_\nu(\hat{\mathbf{n}}){}^{\mathrm{beam-convolved}} = B_\ell(\theta_{0.9 \, \mathrm{arcmin} }) T^i_\nu(\hat{\mathbf{n}}) +N^i_\nu(\hat{\mathbf{n}}),
\ee
where $N^i_\nu(\hat{\mathbf{n}})$ is a random map with power spectrum given by $N_\ell^\nu$, and is not included in the noiseless simulations.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.5} % Adjust row height
    \begin{tabular}{|>{\centering\arraybackslash}m{3cm}|m{7cm}|m{4cm}|}
        \hline
        \textbf{Component} & \textbf{Layer} & \textbf{Output Shape} \\
        \hline
        \multicolumn{3}{|l|}{\textbf{Input}: (4, 128, 128)} \\
        \hline
        
        \multicolumn{3}{|l|}{\textbf{Encoder}} \\
        \hline
        Conv & Conv ($32\times3\times3$) + ReLU + BatchNorm & (32, 128, 128) \\
        Conv & Conv ($32\times3\times3$) + ReLU + BatchNorm & (32, 128, 128) \\
        AvgPool & AvgPool ($2\times2$) & (32, 64, 64) \\
        \hline
        Conv & Conv ($64\times3\times3$) + ReLU + BatchNorm & (64, 64, 64) \\
        Conv & Conv ($64\times3\times3$) + ReLU + BatchNorm & (64, 64, 64) \\
        AvgPool & AvgPool ($2\times2$) & (64, 32, 32) \\
        \hline
        $\cdots$ & $\cdots$ & $\cdots$ \\
        \hline
        Conv & Conv ($512\times3\times3$) + ReLU + BatchNorm & (512, 4, 4) \\
        AvgPool & AvgPool ($2\times2$) & (512, 2, 2) \\
        \hline
        \multicolumn{3}{|l|}{\textbf{Bottleneck}} \\
        \hline
        Conv & Conv ($1024\times3\times3$) + ReLU + BatchNorm & (1024, 2, 2) \\
        Conv & Conv ($1024\times3\times3$) + ReLU + BatchNorm & (1024, 2, 2) \\
        \hline
        \multicolumn{3}{|l|}{\textbf{Decoder}} \\
        \hline
        Inv-Conv & Inv-Conv ($512\times3\times3$) & (512, 4, 4) \\
        Concatenate & Concatenate $X_6$ & (1024, 4, 4) \\
        Conv & Conv ($1024\times3\times3$) + ReLU + BatchNorm & (1024, 4, 4) \\
        \hline
        Inv-Conv & Inv-Conv ($256\times3\times3$) & (256, 8, 8) \\
        Concatenate & Concatenate $X_5$ & (512, 8, 8) \\
        Conv & Conv ($512\times3\times3$) + ReLU + BatchNorm & (512, 8, 8) \\
        \hline
        Inv-Conv & Inv-Conv ($128\times3\times3$) & (128, 16, 16) \\
        Concatenate & Concatenate $X_4$ & (256, 16, 16) \\
        Conv & Conv ($256\times3\times3$) + ReLU + BatchNorm & (256, 16, 16) \\
        \hline
        Inv-Conv & Inv-Conv ($64\times3\times3$) & (64, 32, 32) \\
        Concatenate & Concatenate $X_3$ & (128, 32, 32) \\
        Conv & Conv ($128\times3\times3$) + ReLU + BatchNorm & (128, 32, 32) \\
        \hline
        Inv-Conv & Inv-Conv ($32\times3\times3$) & (32, 64, 64) \\
        Concatenate & Concatenate $X_2$ & (64, 64, 64) \\
        Conv & Conv ($64\times3\times3$) + ReLU + BatchNorm & (64, 64, 64) \\
        \hline
        Inv-Conv & Inv-Conv ($32\times3\times3$) & (32, 128, 128) \\
        Concatenate & Concatenate $X_1$ & (64, 128, 128) \\
        Conv & Conv ($32\times3\times3$) + ReLU + BatchNorm & (32, 128, 128) \\
        \hline
        \multicolumn{3}{|l|}{\textbf{Final Output}} \\
        \hline
        Conv & Conv ($1\times1\times1$) & (1, 128, 128) \\
        \hline
    \end{tabular}
    \caption{U-Net Architecture for Image Segmentation}
    \label{tab:unet_architecture}
\end{table}

Following \cite{mccarthy24_ml}, I train the NN to learn the ILC residuals using this loss function
\be
L=\frac{1}{N_i}\sum_{\hat{\mathbf{n}}}\left(\hat {\Delta T^{\rm{ILC}}}(\hat{\mathbf{n}})-\Delta T^{\rm{ILC}}(\hat{\mathbf{n}})\right)^2.
\ee
We normalize the loss with $N_i = \sum_{\hat{\mathbf{n}}} \Delta T_i^{ilc}(\hat{\mathbf{n}})$, the average ILC residual over pixels for each patch, to penalize the patches that are poorly reconstructed. I train each epoch using 32 batches and for 256 epochs. I use a U-Net model \cite{unet}, whose architecture is summarized in Table~\ref{tab:unet_architecture}.

\subsection{Next Steps}
First, I will reproduce the results obtained in \cite{mccarthy24_ml} for the NN described above. There are several changes to the architecture and training of the NN I can explore . First, I will use group normalization instead of batch normalization because the dataset shares features among different frequency channels. This motivates a standardization across ``groups`` of channels. Moreover, it is less dependent on the batch size and performs consistently well even with small batches, which is useful for the non-uniformly distributed dataset. This could increase stability and improve the NN performance. Second, I will consider the following loss function:
\begin{equation}
    L=\frac{1}{N_i}\sum_{\hat{\mathbf{n}}}\bigg[ \left(\hat {\Delta T^{\rm{ILC}}}(\hat{\mathbf{n}})-\Delta T^{\rm{ILC}}(\hat{\mathbf{n}})\right)^2 - \frac{(\mu_0(\hat{\mathbf{n}})-\hat {\Delta T^{\rm{ILC}}}(\hat{\mathbf{n}}))^2}{\sigma_0^2(\hat{\mathbf{n}})} \bigg]
\end{equation}
where $\mu_0((\hat{\mathbf{n}}))$ and $\sigma_0^2((\hat{\mathbf{n}}))$ are the mean and variance, as a function of pixel, of the input maps in the training set. The motivation for this additional term is to alleviate biases in the NN predictions at the tails of the output distributions.

After training, there are several metrics we can compute to evaluate the network performance. For instance, I will calculate the per-patch mean square error (MSE) of the NN-corrected ILC reconstruction and compare it to the ILC estimate to show how much improvement the NN contributes. As discussed in \cite{mccarthy24_ml}, I expect to see position-dependent trends in the MSE, specifically that the error increases at low-Galactic latitudes which are closer to the Galactic plane. These regions of the CMB are harder to clean due to much larger contamination. I can also test the generalizability of the model to foreground models not seen in the training data by evaluating the NN on more complex PySM3 foreground simulations: (\texttt{$d_X, s_X, ...$} for X$>$1).

To improve upon the hybrid-ML solution found by the NN above, I will perform scale-dependent cleaning for each multipole of the reconstruction by including additional information from all other multipoles. This ensures that the procedure remains signal independent because different scales of the CMB signal are uncorrelated. To do this, I will generate patches of the input and output data defined in Eqn.~\ref{eq:nn_output} and Eqn.~\ref{eq:freq_diff} for each $\ell$ bin, with a bin resolution chosen to minimize ILC bias. Here, rather than dividing the sky map into quadrants based on Galactic coordinates, I will divide the dataset into four subsets based on multipole, reserving one for testing. Then, I will add a new channel in the NN input, $T_i^{\ell*}$, defined as the patches constructed with all multipoles excluding those of interest (denoted $\ell*$). \helen{is it okay to cross-use these data for training/testing?}. Hence, the augmented NN input will be a concatenation of the original input, $\hat{F}_i$, and $T_i^{\ell*}$:
\begin{equation}
    \hat{F}_i^{\ell} \equiv (\hat{F}_i, T^{\ell*})  = \bigg( \bigg[ (F_i^{\ell}+N_i^{\ell}) - \sum_i w_i^{\ell} (F_i^{\ell}+N_i^{\ell}) \bigg], T^{\ell*} \bigg)
    \label{eq:new_nn_input}
\end{equation}
where $i$ denotes the frequency band and $\ell$ labels the multipole (or band of multipoles) we wish to clean. 
% Recall that the terms in brackets contain information from the scale of interest but remains signal-independent by construction.
\helen{Would it make more sense here to use the HILC weights \& reconstruction for each multipole?}.

% The output of the new NN is then
% \begin{equation}
%     \tilde{f}(F, N)_{\ell} \equiv \Delta \hat{T}^{\rm{HILC}}_{\ell} = \hat{T}^{\rm{HILC}}_{\ell} - T^{coi}_{\ell}
% \end{equation}
% \helen{Where $\hat{T}^{\rm{HILC}}_{\ell}$ is the HILC pixel-space map reconstruction of a specific alm bin}.

Similar to the tests conducted for the original NN, I can evaluate the MSE of the results given by this model as a function of $\ell$ to analyze patterns in the performance for scales that are harder to clean than others. For instance, lower multipoles are more dominated by foreground contamination and thus will exhibit weaker reconstruction. 


\bibliography{references}
\end{document}