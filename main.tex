\PassOptionsToPackage{numbers,sort&compress}{natbib}

\documentclass[preprintnumbers,amsmath,amssymb,prd, notitlepage,nofootinbib, superscriptaddress]{revtex4-2}

\usepackage{amsfonts,amssymb,amsmath}

\usepackage{gensymb}

\usepackage{color}

\usepackage{graphicx}

\usepackage{multirow}

\usepackage[utf8]{inputenc}

\usepackage{aas_macros}

\usepackage{hyperref}

\usepackage{array}

\usepackage{booktabs}

\usepackage[normalem]{ulem}

\definecolor{lightgray}{gray}{0.95}

\usepackage{tikz}

\usetikzlibrary{shapes.geometric, arrows, positioning, decorations.pathreplacing, calc}

\newcommand{\Planck}{{\it Planck}~}
\newcommand{\be}{\begin{equation}}        
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}[1]{\begin{align}
#1
\end{align}}
\newcommand{\helen}[1]{\textcolor{purple}{#1}}

\begin{document}

\title{Signal-Preserving Machine Learning for CMB Foreground Reconstruction: Multi-frequency and Inter-scale Approaches}

% \author{Helen Shao}
% \affiliation{Department of Physics, Princeton University}
% \email{helen.shao@cfa.harvard.edu}

% \author{Fiona McCarthy}
% \affiliation{Department of Applied Mathematics and Theoretical Physics, University of Cambridge}

% \author{Miles Cranmer}
% \affiliation{Department of Applied Mathematics and Theoretical Physics, University of Cambridge}

% \author{Blake Sherwin}
% \affiliation{Department of Applied Mathematics and Theoretical Physics, University of Cambridge}

\date{\today}

\begin{abstract}
Accurate measurement of Cosmic Microwave Background (CMB) B-mode polarization, a key probe of inflationary physics, is hindered by complex astrophysical foreground contamination. While traditional component separation methods like the Internal Linear Combination (ILC) can mitigate foregrounds to high accuracy, they require multiple frequency channels and are limited to second-order statistics. We present novel signal-preserving machine learning frameworks for foreground reconstruction using single-frequency observations by leveraging inter-scale correlations within foreground maps, where small-scale information reconstructs large-scale contamination. We also combine multi-frequency and multi-scale approaches, using frequency-difference maps and inter-scale information as parallel inputs. Using realistic simulations of polarized Galactic dust emission from the DustFilaments model, we demonstrate improved foreground removal compared to baseline methods, with $\sim$50\% harmonic correlation for the single-frequency inter-scale model. Including additional information from temperature and E modes achieves further improvements up to $\sim80\%$ in correlation. We validate the signal-preserving property of the reconstruction through cross-correlation analysis and demonstrate the method's significant improvements in mean squared error compared to the ILC solution.
\end{abstract}

\maketitle

\section{Introduction}\label{sec:introduction}

Primordial B-mode polarization of the Cosmic Microwave Background (CMB) offers a unique observational signature of inflationary gravitational waves and serves as a crucial probe of the physics that govern the early universe \citep{zaldarriaga1997polarization, kamionkowski1997detecting, seljak1997signature}. At large angular scales ($\ell \lesssim 100-200$), the CMB B-mode power spectrum constrains the inflationary energy scale through the tensor-to-scalar ratio, $r$ \citep{baumann2009inflation, hu1997cmb}. The current direct constraint on this parameter is an upper bound of $r < 0.036$ at 95\% confidence, set by the BICEP/Keck Array experiment \citep{2018SPIE10708E..07H}. Improving the statistical significance of this bound, however, is severely complicated by the overwhelming presence of Galactic foreground contamination across all frequencies, which must be accurately modeled and removed to isolate the faint primordial signal \citep{bk2021, Campeti_2022}. 

At large angular scales, the dominant Galactic contamination to CMB polarization maps is sourced by diffuse synchrotron emission at low frequencies ($\lesssim 100 GHz$) and thermal dust emission at high frequencies ($\gtrsim 200 GHz$) \citep{planck2016foregrounds, kamionkowski2016}. In this work, we will focus on the separation and removal of the latter component. Dust grains in the Milky Way, including ices, silicates, and polycyclic aromatic hydrocarbons, are heated by high energy photons and re-radiate in the infrared~\citep{draine2011thermal,thorne2017}. The Galactic magnetic field also aligns the spin of these elongated grains, causing their emission to be polarized~\citep{draine2009,fraisse2009}. In the CMB B-modes power spectrum, this polarization washes out any potential inflationary signal \citep{planck2016foregrounds, planck2018componentsep} and needs to be carefully cleaned. However, these foregrounds exhibit complex spatial structures, non-Gaussian statistics, and frequency-dependent spectral energy distributions (SEDs) that make it difficult to separate them from the CMB signal. Incomplete knowledge of these properties in the modeling and uncertainties in the removal of this foreground directly translates into a residual "foreground noise" that sets a floor on the achievable uncertainty in $r$ constraints, well above the $\sigma(r) \sim 10^{-3}$ goal of future CMB experiments~\citep{Simmons_2025}. Hence, effectively separating and mitigating this component is crucial for maximizing the scientific return from existing and future CMB maps, such as those from the Atacama Cosmology Telescope (ACT)~\cite{Choi_2020}, the South Pole Telescope (SPT)~\cite{Carlstrom_2011}, the Simons Observatory (SO)~\cite{SimonsObservatory2019}, and LiteBIRD~\cite{2023PTEP.2023d2F01L}.

Internal Linear Combination (ILC) ~\cite{bennett2003firstyear, eriksen2004ilc, delabrouille2009needletilc} is one of the most widely used class of methods for this objective. It exploits the frequency dependence of the different components present in CMB maps, which can be quantified through their spectral energy distributions (SEDs). While the astrophysical and Galactic foregrounds have correlated SEDs, the near-perfect blackbody spectrum of the primary CMB is invariant across all frequencies and uncorrelated with the foregrounds. ILC methods leverage this relationship by constructing a linear combination of multi-frequency maps that optimally isolates and removes sources of contamination. Crucially, the weights of this combination are computed to minimize the variance of the resulting map, which gives the optimal reconstruction of the CMB signal under the assumption that it is purely Gaussian. The advantages of this method are that it is agnostic to the foreground model, offering a purely statistical removal of the unwanted contamination. Moreover, by using the fact that the primary CMB is uncorrelated with the foregrounds in all frequency bands, the ILC solution is explicitly constrained to contain unit response to the CMB signal. This ensures that the reconstruction is \textit{signal preserving} --- i.e., it fully recovers the primary CMB independent of the nature of the other components.

While ILC offers several advantages -- analytical tractability, no requirement for foreground templates, and signal preservation -- it is important to outline its limitations.
First, its effectiveness critically depends on access to multiple frequency channels with sufficient spectral coverage to resolve the different SEDs of the components present in the observations. Since CMB experiments measure in a finite number of wavelength bands, the spectral degrees of freedom of complex foregrounds cannot be perfectly sampled, leading to residual contamination. This limitation becomes particularly problematic when foregrounds exhibit spatially varying SEDs, as standard ILC applies the same weights over its domain of operation (full/partial sky, spatial/harmonic space) and cannot completely null foregrounds with simultaneously frequency-varying properties and spatially-varying spectra \citep{1997PhRvL..78.2058K, PhysRevLett.78.2054}. Variants such as the Needlet Internal Linear Combination (NILC)~\citep{needlet} address this by minimizing the variance in separate spatially- and harmonically-localized domains, but the optimality of NILC remains dependent on the data being approximately isotropic and Gaussian within each of these domains.

Second, the ILC relies solely on second-order statistics (in the case of spatial ILC, it is the frequency-space variance of the observed maps), neglecting higher-order correlations that are abundant in Galactic foregrounds. When foreground statistics are anisotropic and non-Gaussian across the sky, standard global component separation of the ILC is suboptimal. This drawback is particularly problematic for foreground sources with correlated structures in harmonic and real space \citep{planck2016interstellar}. For instance, turbulent processes affecting the behavior of dust grains in the Milky Way interstellar medium are highly non-Gaussian and anisotropic, with dust emission strength increasing nearer the Galactic plane \citep{planck2016foregrounds}.

These limitations have motivated recent efforts to incorporate machine learning (ML) methods into CMB component separation, due to their potential for modeling complex, non-linear relationships and capturing the non-Gaussian features missed by ILC \citep{munchmeyer2019planck, petroff2020, krachmalnicoff2021deepmc, mccarthy24_ml}. However, a significant challenge for ML-based approaches is their sensitivity to the information provided by simulations that are used as inputs for training. Given the inherent uncertainties and approximations in incomplete foreground modeling, mismatches between simulations and real data can introduce simulation bias into the recovered CMB signal \citep{alsing2019simulationbias}. As a result, ML models trained directly on full observational maps that contain the primary CMB signal risk inadvertently learning and removing part of the cosmological signal. This makes the ML reconstruction uninterpretable and potentially unreliable for subsequent cosmological inference on real observations. Therefore, ensuring signal preservation and quantifying uncertainties remain critical concerns for any ML-based component separation approach.

To address this challenge of simulation bias while preserving the signal, \cite{mccarthy24_ml} presented a novel framework that blinds the ML algorithm to the CMB signal of interest, thereby ensuring the final reconstruction respects the signal-preserving property of the ILC solution. The key is to exploit the spectral properties of the distinct components present in the observed maps: foregrounds have frequency-dependent SEDs while the CMB has an identical amplitude across all frequencies. Hence, by taking differences between frequency channels, the CMB signal cancels out, leaving only foreground and noise contributions. Using these frequency-difference maps as inputs, the authors then train a neural network to predict ILC residuals --- the difference between ILC-reconstructed CMB and true CMB --- using the frequency-difference information as input. Since the CMB signal is absent from both the input (frequency differences) and target (ILC residuals), the network learns \textit{only} foreground properties. The output of the network is then subtracted from the ILC reconstruction to yield an improved, non-linear estimate of the CMB signal. With this ML model, \cite{mccarthy24_ml} produces \textit{unbiased} CMB reconstructions with variances that are up to five times lower than that of ILC. While this approach is powerful, it inherits the multi-frequency data requirement of the ILC, thereby limiting its applicability to experiments with sparse frequency coverage or single-frequency observations.

We extend the signal-preserving ML framework to address the challenge of single-frequency observations by leveraging the intrinsic inter-scale correlations within astrophysical foreground maps. The key insight is that while primordial CMB modes exhibit statistical independence across angular scales (consistent with a Gaussian random field), Galactic foreground emission processes induce significant correlations between structures at different angular scales \citep{lazarian2000turbulence, cho2002mhd}. This property, which arises from the underlying physics of the interstellar medium, critically distinguishes foreground components from the primary CMB. This, in principle, enables the use of small-scale information to predict large-scale Galactic properties and remove its contamination \citep{kovetz}. For instance, \cite{philcox2018} used the statistical anisotropy of Galactic dust B-modes to construct a bipolar-spherical harmonic estimator that can clean dust-dominated CMB maps with forecasted constraints on $r \sim 0.001$ at 2$\sigma$. 

In this paper, we combine the signal-preserving technique developed in \cite{mccarthy24_ml} with the use of inter-scale statistical correlations within foregrounds to reconstruct and remove Galactic dust polarization. Specifically, we investigate how the statistical information contained in small angular scales ($\ell > 200$), where foregrounds dominate and primordial B-mode power is negligible, can be used to predict large-scale foreground contamination ($\ell < 200$) that obscures the signal of interest. We emphasize that since the primary CMB modes are statistically independent across angular scales, using small-scale information to predict and remove large-scale foregrounds preserves the cosmological signal by construction. We demonstrate the effectiveness of this method using realistic simulations of Galactic dust emission from the DustFilaments model \citep{Herv_as_Caimapo_2022}, and show that it can achieve mean spatial correlations of $0.45$ and normalized cross-power spectrum correlations of $0.49$ for single-frequency inter-scale models. We later augment the network inputs with complementary temperature and E-mode polarization maps to improve the foreground reconstruction. Using this additional information, we achieve mean spatial correlations of $0.79$ and normalized cross-power spectrum correlations of $0.80$. Finally, we explore a hybrid approach that combines the frequency-difference and inter-scale information, and show that this hybrid multi-frequency inter-scale model achieves CMB reconstruction with mean squared error reductions of $2534\times$ (8-channel configuration) and $3888\times$ (16-channel configuration) compared to the ILC baseline, with normalized cross-power spectrum correlations exceeding $0.99999$.

This paper is structured as follows. We first present the inter-scale foreground reconstruction framework and notation in Section~\ref{sec:interscale}. Next, we describe the simulation data used to demonstrate the proposed method in Section~\ref{sec:simulations}. We explain the neural network architecture and training procedures in Section~\ref{sec:methods}, detailing variations of the inter-scale reconstruction with augmentations of the network input channels to include temperature, E-mode polarization, and multi-frequency information. We then describe the validation metrics used to evaluate the performances of these networks in Section~\ref{sec:metrics}. In Section~\ref{sec:results}, we present the network predictions and comprehensive evaluations of the resulting foreground and CMB reconstructions, including spatial and harmonic space correlation and accuracy metrics. Finally, we conclude in Section~\ref{sec:discussion} by discussing the implications, limitations, and future directions of this work.

\section{Signal-Preserving Inter-Scale Foreground Reconstruction}\label{sec:interscale}

We begin by presenting the formalism of signal-free reconstruction with ML, and apply it to the case of inter-scale component separation with the goal of cleaning large-scale B-mode polarization. Inspired by the framework introduced in \cite{mccarthy24_ml}, we use the following observation model for multi-frequency CMB B-mode polarization maps:
\begin{equation}
B_i(\hat{\mathbf{n}}) =S^{\mathrm{coi}}(\hat{\mathbf{n}}) + F_i(\hat{\mathbf{n}})\label{eq:general_obs_model}
\end{equation}
where $i$ indicates a frequency channel, $\hat{\mathbf{n}}$ is a unit vector on the observation sphere, $S^{\mathrm{coi}}(\hat{\mathbf{n}})$ is the component of interest and $F_i(\hat{\mathbf{n}})$ are the foregrounds, which are uncorrelated with $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. The goal is to design a neural network $\mathcal{N}$ to separate and remove $F_i(\hat{\mathbf{n}})$ from $B_i(\hat{\mathbf{n}})$ and reconstruct $S^{\mathrm{coi}}(\hat{\mathbf{n}})$ without biasing or removing parts of $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. The key to ensuring that $\mathcal{N}$ obeys this signal-preserving property is two-fold:
\begin{itemize}
    \item \textbf{Signal-free inputs:} First, inputs of $\mathcal{N}$ must be uncorrelated with $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. This ensures the network cannot learn anything about $S^{\mathrm{coi}}(\hat{\mathbf{n}})$ from the inputs and thereby does not have access to information that can potentially bias $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. This includes simulation misspecification and incomplete physical models in the data. However, the inputs must to be correlated with the components $F_i(\hat{\mathbf{n}})$ that one wishes to remove.
    \item \textbf{Signal-free targets:} Second, the network is constrained to predict targets that are uncorrelated with $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. This enables one to subtract the network prediction from  $B_i(\hat{\mathbf{n}})$ without inadvertently removing parts of  $S^{\mathrm{coi}}(\hat{\mathbf{n}})$. If the network has inherent biases or inaccurate predictions, its predictions would not affect the signal we wish to preserve.
\end{itemize}

This dual constraint ensures that even if the network's learned mapping is imperfect or if simulations contain modeling errors, the primary CMB signal remains unbiased for cosmological inference down the line. The mathematical manifestation of this unbiasedness is that the cross-power spectrum between the reconstructed signal $\hat{S}^{\mathrm{coi}}$ and the true signal $S^{\mathrm{coi}}$ equals the auto-power spectrum of the true signal:
\begin{equation}
    \langle \hat{S}^{\mathrm{coi}} S^{\mathrm{coi}} \rangle = \langle S^{\mathrm{coi}} S^{\mathrm{coi}} \rangle,
    \label{eq:unbiasedness_condition}
\end{equation}
which we verify in our results (see Section~\ref{sec:metrics}). This condition holds by construction because the network operates only on signal-free inputs and predicts signal-free targets, ensuring that the signal propagates through the reconstruction pipeline without modification.

While one is free to choose inputs and targets that satisfy these criteria, in this work, we construct them by exploiting the relationships between large- and small-scale structures within the different components of the observation model. Specifically, we aim to separate the signal from the foregrounds at large angular scales where primordial B-modes are expected to be detected, using only information from the independent small angular scales where foregrounds dominate and the primordial CMB signal is negligible. 

Recall that the primary CMB signal, $S(\hat{\textbf{n}})$, can be described as a statistically isotropic Gaussian random field, and consequently exhibits statistical independence between different angular multipoles $\ell$. In contrast, Galactic foregrounds in the form of polarized thermal dust emission is driven by processes like turbulence in the magnetized interstellar medium that couple various scales\citep{lazarian2000turbulence, cho2002mhd}. This is consistent with the filamentary nature of dust distribution in the Milky Way which create coherent structures across multiple angular scales. Consequently, $F_i(\hat{\textbf{n}})$ significant correlations between structures observed at different harmonic modes. We can thus exploit these inter-scale relations to train a network on information at one set of scales, while preserving the primary CMB signal in the other independent set of target scales. 

To this end, we filter the maps in harmonic space and decompose the observation model into two sets: 
\begin{align}
B_{L,i}(\hat{\mathbf{n}}) &= S_L(\hat{\mathbf{n}}) + F_{L,i}(\hat{\mathbf{n}}) \label{eq:B_large} \\
B_{S,i}(\hat{\mathbf{n}}) &= S_S(\hat{\mathbf{n}}) + F_{S,i}(\hat{\mathbf{n}}) \label{eq:B_small}
\end{align}
We choose to filter at a scale cut of $\ell=200$ in order to isolate the scales that are most sensitive to inflationary signals. Hence, $B_{L,i}(\hat{\mathbf{n}})$ denotes the maps containing only large-scale information at $\ell < 200$ and $B_S(\hat{\mathbf{n}})$ denotes the small scales at $\ell > 200$. In the following sections, we describe the various configurations of this core framework to perform CMB reconstruction. Again, since we are concerned with recovering the component that can constrain inflationary physics, the component of interest that needs to be preserved in all training configurations is the primary CMB B-mode polarization: $S_L^{\textrm{coi}}(\hat{\mathbf{n}})$.

\subsection{Inter-Scale Learning: Single-Frequency Inputs}\label{sec:interscale_single_frequency}

We begin with inter-scale reconstruction at a \textit{single frequency}. In this case, the channel $i$ in the decompositions described in Eqn.~\ref{eq:B_large} and Eqn.~\ref{eq:B_small} can be eliminated and we choose to fix it to 270 GHz. We start off with using only B-mode polarization data as the input, and later augment it with additional channels of independent information to improve the predictions.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/diagram_sing_freq.pdf}
    \caption{Schematic diagram of our inter-scale machine learning framework for signal-preserving foreground reconstruction. \textbf{(a)} The primary CMB signal is added with foreground observations from a single frequency to give \textbf{(b)} a contaminated map $B(\hat{\mathbf{n}})$. Since this map contains the signal of interest, it cannot be directly fed to the network. However, we can decompose it into large-scale ($B_L$, $\ell < \ell_{\text{cut}}$) and small-scale ($B_S$, $\ell > \ell_{\text{cut}}$) components. \textbf{(c)} The filtered small scale map is then fed to the network as a s signal free input. \textbf{(d)} The  network is trained to predict the large-scale foreground component $\hat{F}_L$. \textbf{(e)} $\hat{F}_L$ is then subtracted from the original large-scale map $B_L$ (which contains $S_L +  F_L$) to produce the NN-cleaned map $B^{\text{pred}}_L$. Since the input $F_S$ is statistically independent of the large-scale CMB signal $S_L$, the prediction $\hat{F}_L$ is also independent of $S_L$, ensuring the cosmological signal is preserved in the final map.}
    \label{fig:interscale-pipeline}
\end{figure}

\subsubsection{Input: Small-scale B-modes}\label{sec:interscale_b}
First, we use $B_{S}(\hat{\mathbf{n}})$ as the signal-free input and train the neural network to predict the  large-scale foregrounds, $F_{L}(\hat{\mathbf{n}})$. Notice that this configuration obeys the two constraints stated in Sec~\ref{sec:interscale_b}, since the primary CMB components $S_L$ and $S_S$ are statistically independent (as expected for a Gaussian random field) and the foregrounds (F) are uncorrelated with the signal. Moreover, the target only contains the foregrounds. However, this setup still yields useful information for component separation because the foreground components $F_L$ and $F_S$ are correlated due to the underlying physical processes that couple them in the Galactic field. 

In practice, during training, we remove the small-scale primary CMB component $S_S(\hat{\mathbf{n}})$ from the network input. Given that the CMB is a Gaussian random field, $S_S(\hat{\mathbf{n}})$ behaves as a noise-like contaminant that is statistically independent of both $F_S(\hat{\mathbf{n}})$ and $F_L(\hat{\mathbf{n}})$. Hence, it provides no useful information that could aid the reconstruction, and including it could potentially hinder the network's ability to learn the foreground inter-scale correlations. Isolating $F_S(\hat{\mathbf{n}})$ as input focuses the network on the characteristics of the foregrounds, which are the target of the reconstruction. The network prediction, which we denote as $\hat{F}_L(\hat{\mathbf{n}})$, is thus a function of only the small-scale foregrounds:
\begin{equation}
\hat{F}_L(\hat{\mathbf{n}}) = \mathcal{N}(B_S(\hat{\mathbf{n}})) = \mathcal{N}(F_S(\hat{\mathbf{n}})) \label{eq:F_L_prediction}
\end{equation}
Notice that both the input and targets are uncorrelated with the signal of interest for the reasons described above.

We then later subtract the prediction $\hat{F}_L(\hat{\mathbf{n}})$ from $B_L(\hat{\mathbf{n}})$ to obtain the neural network's reconstruction of $S^{\textrm{coi}}_L(\hat{\mathbf{n}})$:
\begin{equation}
\hat{S}^{\textrm{coi}}_L(\hat{\mathbf{n}}) = B_L(\hat{\mathbf{n}}) - \hat{F}_L(\hat{\mathbf{n}}) = S_L(\hat{\mathbf{n}}) + F_L(\hat{\mathbf{n}}) - \mathcal{N}(F_S(\hat{\mathbf{n}})) \label{eq:B_L_reconstruction}
\end{equation}

This first configuration investigates the extent to which correlations exist between $F_S$ and $F_L$, and whether a network can capture these correlations via a non-linear mapping to accurately recover the independent primary CMB signal. Fig.~\ref{fig:interscale-pipeline} is a schematic of this setup, illustrating the preservation of large-scale primary CMB B-modes, $\hat{S}^{\textrm{coi}}_L(\hat{\mathbf{n}})$, throughout the reconstruction pipeline.

\subsubsection{Inputs: Small-scale B-modes + Temperature + E-Modes}\label{sec:interscale_t_e}

To improve the foreground prediction, we augment the inputs with additional information that is uncorrelated with the COI but potentially correlated with the large-scale foregrounds. To potentially improve the predictive power for the inter-scale objective, we explore augmenting the input data with additional information from statistically independent components of the primary CMB. Specifically, in this second case, we include CMB temperature maps $T(\hat{\mathbf{n}})$ and E-mode polarization maps $E(\hat{\mathbf{n}})$ as additional channels of the network inputs:
\begin{align}
T(\hat{\mathbf{n}}) &= S_T(\hat{\mathbf{n}}) + F_T(\hat{\mathbf{n}}) \label{eq:T_map} \\
E(\hat{\mathbf{n}}) &= S_E(\hat{\mathbf{n}}) + F_E(\hat{\mathbf{n}}) \label{eq:E_map}
\end{align}
Fig.~\ref{fig:augmented_training_example} shows an example of the training data for the inter-scale foreground prediction objective with T/E mode augmentation. The network prediction in this configuration is:
\begin{equation}
\hat{F}_L^B(\hat{\mathbf{n}}) = \mathcal{N}_{\text{aug}}(B_S(\hat{\mathbf{n}}), T(\hat{\mathbf{n}}), E(\hat{\mathbf{n}})) = \mathcal{N}_{\text{aug}}(F_S^B(\hat{\mathbf{n}}), F_T(\hat{\mathbf{n}}), F_E(\hat{\mathbf{n}})) \label{eq:F_L_aug_prediction}
\end{equation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/TEB-patches.png}
    \caption{Example training data for the inter-scale foreground prediction objective with T/E mode augmentation. From left to right: (a) Input patch containing small-scale B-mode information ($T_S^B$, $\ell > 200$). (b) Input patch containing the full E-mode polarization map ($E(\hat{\mathbf{n}})$, all $\ell$). (c) Input patch containing the full Temperature intensity map ($I(\hat{\mathbf{n}})$, all $\ell$). (d) Corresponding target patch containing the large-scale B-mode foreground component ($F_L^B$, $\ell < 200$). Panels (a), (b), and (c) are concatenated channel-wise as input to the network. All colorbars are labeled in units of $\mu$K.}
    \label{fig:augmented_training_example}
\end{figure}

Critically, this augmentation maintains signal preservation for the large-scale B-mode CMB signal $S_L^B(\hat{\mathbf{n}})$. This relies on the property of the primary CMB that its temperature ($S_T$), E-mode ($S_E$), and primordial B-mode ($S^B$) components are statistically independent Gaussian random fields. Therefore, providing the full $T(\hat{\mathbf{n}})$ and $E(\hat{\mathbf{n}})$ maps (which contain primary information, $S_T$ and $S_E$, that overlap in scale with the target) as input to the network does not introduce information that is correlated with $S^{\textrm{coi}}_L(\hat{\mathbf{n}})$.
% The network's prediction $\hat{F}_L^B(\hat{\mathbf{n}})$ remains statistically independent of $S_L^B(\hat{\mathbf{n}})$, ensuring that $\langle T_L^{B, \text{pred}}(\hat{\mathbf{n}}) S_L^B(\hat{\mathbf{n}}) \rangle = \langle S_L^B(\hat{\mathbf{n}}) S_L^B(\hat{\mathbf{n}}) \rangle$.

Given this statistical independence, we discard the primary CMB components $S_T$ and $S_E$ from the temperature and E-mode inputs during training, as they act as random noise that is not correlated with neither the target component nor the component we wish to preserve. 

% The network's prediction $\hat{F}_L^B(\hat{\mathbf{n}})$ remains statistically independent of $S_L^B(\hat{\mathbf{n}})$, ensuring that:
% \begin{equation}
% \langle \hat{B}_L(\hat{\mathbf{n}}) S_L^B(\hat{\mathbf{n}}) \rangle = \langle S_L^B(\hat{\mathbf{n}}) S_L^B(\hat{\mathbf{n}}) \rangle \label{eq:signal_preservation}
% \end{equation}.
The final cleaned reconstruction is:
\begin{equation}
\hat{S}_L(\hat{\mathbf{n}}) = B_L(\hat{\mathbf{n}}) - \hat{F}_L^B(\hat{\mathbf{n}}) = S_L(\hat{\mathbf{n}}) + F_L(\hat{\mathbf{n}}) - \mathcal{N}_{\text{aug}}(F_S^B(\hat{\mathbf{n}}), F_T(\hat{\mathbf{n}}), F_E(\hat{\mathbf{n}})) \label{eq:B_L_aug_reconstruction}
\end{equation}

Again, we emphasize that this approach relies only on \textit{single frequency} maps.

\subsection{Frequency-difference + Inter-scale UNet}\label{sec:multifreq_interscale}
Next, we explore a hybrid approach that combines inter-scale learning with the multi-frequency framework introduced in \cite{mccarthy24_ml}. This configuration leverages the correlation between small-scale and large-scale structures in the observation maps, as well as the correlations of the SEDs between different frequency channels. Adapting the method from \cite{mccarthy24_ml}, we use ``frequency-difference maps`` as input to the network. These are computed as the linear combination of the foreground components given by the ILC weights:
\begin{equation}
    \hat{F}^{\rm{ILC}}_i = B_i - \hat{S}^{\mathrm{ILC}} = F_i - \sum_j w_j F_j
    \label{eq:freq_diff}
\end{equation}
The $\hat{F}_i$ maps can be interpreted as the ILC estimation of the foregrounds based on their correlations in across different frequencies. Therefore, $\hat{F}_i$ is uncorrelated with the signal of interest. Consequently, these $\hat{F}_i$ inputs satisfy the first constraint of the signal-free property.

To fulfill the second constraint, we again follow \cite{mccarthy24_ml} and train the network to predict signal-free targets --- the ILC residuals. These are defined as:
\begin{equation}
    \hat{\Delta B}^{\mathrm{ILC}} = B_i - \hat{B}^{\mathrm{ILC}}
    \label{eq:ilc_residuals}
\end{equation}
In the above equation, one recognizes that the ILC solution, $\hat{B}^{\mathrm{ILC}}$, contains the full primary CMB signal. Consequently, the signal of interest is removed upon subtraction from the observed map, $B_i$, resulting in a signal-free target.

Following the general framework of \cite{mccarthy24_ml}, any signal-preserving reconstruction function must have the form:
\begin{equation}
    f(B^i) = \sum_i w_i B_i + \tilde{f}(F),
    \label{eq:general_signal_preserving}
\end{equation}
where $w_i$ are weights satisfying the ILC condition $\sum_i a_i w_i = 1$ (for CMB B-modes, $a_i = 1$ for all frequencies), and $\tilde{f}$ is a non-linear function operating only on signal-free combinations of the data. In our implementation, we choose $w_i$ to be the ILC weights that minimize variance, and $\tilde{f}$ to be the neural network that predicts ILC residuals.

The neural network improved reconstruction of the signal of interest can then be expressed as:
\begin{equation} #  f(B^i) = S(\hat{\mathbf{n}}) +  \sum_i w_i (F_i) + \tilde f(F).
 B^{\mathrm{pred}}_L =  B^{\mathrm{ILC}}_L - \hat{\Delta B^{\mathrm ILC}}_L = \sum _i w_i B_i - \tilde{f}(\hat{F}^{\mathrm{ILC}}_i, B_S, \ldots),\label{ml_pred_coi}
\end{equation}
where the weights $w_i$ are given by the ILC solution, and $\tilde{f}$ denotes the neural network operating on signal-free inputs (ILC foreground estimates $\hat{F}^{\mathrm{ILC}}_i$, small-scale B-modes $B_S$, and optionally T/E modes). Since the ILC weights satisfy $\sum_i a_i w_i = 1$, the linear term preserves the signal, and since $\tilde{f}$ operates only on signal-free inputs, the entire expression can be written as:
\begin{equation}
 B^{\mathrm{pred}}_L = S_L(\hat{\mathbf{n}}) +  \sum_i w_i (F_i) + \tilde{f}(F),
 \label{eq:ultimate_reconstruction}
\end{equation}
which maintains the signal-preserving property. The key constraint is that $\sum_i a_i c_i = 0$ for any coefficients $c_i$ used to construct signal-free inputs to $\tilde{f}$, ensuring that signal components are completely removed from the network inputs. 

\subsubsection{Input: $\hat{F}^{\rm{ILC}}_i$ + Small-scale B-modes}\label{sec:multifreq_small_b}
We first investigate the combination of frequency-difference maps with small-scale B-modes as inputs to train a network to predict the ILC residuals. This is analogous to the inter-scale learning framework described in Sec~\ref{sec:interscale_b}. Hence, the network prediction can be written as:
\begin{equation}
    \hat{\Delta B^{\mathrm ILC}}_L = \mathcal{N}_{\text{scale}}(B_S, \hat{F}_i)
    \label{eq:multifreq_interscale_pred}
\end{equation}
and the final cleaned reconstruction is:
\begin{equation}
    S_L^{\mathrm{pred}} = B_L^{\mathrm{ILC}} - \hat{\Delta B^{\mathrm ILC}}_L = S_L +  \sum_i w_i (F_i) + \tilde f(F).
    \label{eq:multifreq_interscale_cmb_reconstruction}
\end{equation}

\subsubsection{Input: Small-scale B-modes + Temperature + E-modes}\label{sec:multifreq_teb}
Finally, analogous to Sec~\ref{sec:interscale_t_e}, we explore the combination of frequency-difference maps with small-scale B-modes, temperature, and E-modes as inputs to train a network to predict the ILC residuals. In this case, the network prediction can be written as:

\begin{equation}
    \hat{\Delta B^{\mathrm ILC}}_L = \mathcal{N}_{\text{scale}}(B_S, T, E, \hat{F}_i)
    \label{eq:multifreq_interscale_pred_teb}
\end{equation}
and the final cleaned reconstruction is:
\begin{equation}
    S_L^{\mathrm{pred}} = B_L^{\mathrm{ILC}} - \hat{\Delta B^{\mathrm ILC}}_L = S_L +  \sum_i w_i (F_i) + \tilde f(F).
    \label{eq:multifreq_interscale_cmb_reconstruction_teb}
\end{equation}

In the subsequent section, we will detail the network architectures used to implement this both the single-frequency interscale configuration and the multi-frequency interscale hybrid approach. We will also present the training procedure and the simulation data used to train the neural networks.

% Let $B(\hat{\mathbf{n}})$ be the observed map at a single frequency, containing signal $S(\hat{\mathbf{n}})$ and foreground $F(\hat{\mathbf{n}})$. We can decompose this map into components based on angular scale, for instance, a large-scale map $B_L(\hat{\mathbf{n}})$ containing multipoles $\ell < \ell_{\text{cut}}$ and a small-scale map $B_S(\hat{\mathbf{n}})$ with $\ell > \ell_{\text{cut}}$.
% \begin{align}
%     B_L(\hat{\mathbf{n}}) &= S_L(\hat{\mathbf{n}}) + F_L(\hat{\mathbf{n}}) \\
%     B_S(\hat{\mathbf{n}}) &= S_S(\hat{\mathbf{n}}) + F_S(\hat{\mathbf{n}})
% \end{align}
% Our goal is to estimate the large-scale foreground contamination, $F_L(\hat{\mathbf{n}})$. We propose training an neural network, $\mathcal{N}_{\text{scale}}$, to predict $F_L$ using only the small-scale foreground component $F_S(\hat{\mathbf{n}})$ as input: $\hat{F}_L(\hat{\mathbf{n}}) = \mathcal{N}_{\text{scale}}(F_S(\hat{\mathbf{n}}))$. 

% Since the CMB signal components $S_L(\hat{\mathbf{n}})$ and $S_S(\hat{\mathbf{n}})$ are statistically independent, using $B_S(\hat{\mathbf{n}})$ as input ensures that the prediction $\hat{F}_L(\hat{\mathbf{n}})$ is independent of the large-scale CMB signal $S_L(\hat{\mathbf{n}})$ that we wish to preserve. The neural network learns to exploit the correlation between $F_S(\hat{\mathbf{n}})$ (present in the input $B_S(\hat{\mathbf{n}})$) and $F_L(\hat{\mathbf{n}})$ to make its prediction.

% The final cleaned estimate of the large-scale map would then be:
% \begin{equation}
%     B^{\text{pred}}_L(\hat{\mathbf{n}}) = B_L(\hat{\mathbf{n}}) - \hat{F}_L(\hat{\mathbf{n}}) = (S_L(\hat{\mathbf{n}}) + F_L(\hat{\mathbf{n}})) - \mathcal{N}_{\text{scale}}(S_S(\hat{\mathbf{n}}) + F_S(\hat{\mathbf{n}})).
% \end{equation}
% Again, because the correction term $\hat{F}_L(\hat{\mathbf{n}})$ is independent of $S_L(\hat{\mathbf{n}})$, this reconstruction preserves the large-scale CMB signal by construction, $\langle B^{\text{pred}}_L(\hat{\mathbf{n}}) S_L(\hat{\mathbf{n}}) \rangle = \langle S_L(\hat{\mathbf{n}}) S_L(\hat{\mathbf{n}}) \rangle$. This approach, illustrated schematically in Figure \ref{fig:interscale-pipeline}, offers a pathway to signal-preserving foreground cleaning using potentially only single-frequency data, relying on the statistical differences between the CMB and foregrounds across angular scales rather than across frequencies. 

% Note that when training the neural network with this framework, we remove the small-scale primary CMB component, $S_S(\hat{\mathbf{n}})$, from the input. Given that the CMB is a Gaussian random field, $S_S(\hat{\mathbf{n}})$ is statistically independent of $F_S(\hat{\mathbf{n}})$ and would act as a noise-like contaminant if included, potentially hindering the network's ability to learn the foreground inter-scale correlations. Isolating $F_S(\hat{\mathbf{n}})$ as the input focuses the network on the characteristics of the foregrounds, which are the target of the reconstruction. 

% \subsection{Inter-scale Learning: B-modes + Temperature + E-modes}\label{sec:interscale_single_frequency_temperature_e_mode}

% Formally, let $I(\hat{\mathbf{n}})$ denote the full-sky temperature intensity map and $E(\hat{\mathbf{n}})$ the full-sky E-mode polarization map, both containing information across all angular scales. The small-scale B-mode foreground component, previously denoted $F_S(\hat{\mathbf{n}})$ in Section \ref{sec:single-frequency-ML}, is now specified as $F_S^B(\hat{\mathbf{n}})$ to clarify its B-mode nature. The neural network, now denoted $\mathcal{N}_{\text{aug}}$, is trained to predict the large-scale B-mode foreground component $\hat{F}_L^B(\hat{\mathbf{n}})$ using this augmented set of inputs:
% \begin{equation}
%     \hat{F}_L^B(\hat{\mathbf{n}}) = \mathcal{N}_{\text{aug}}(F_S^B(\hat{\mathbf{n}}), I(\hat{\mathbf{n}}), E(\hat{\mathbf{n}})).
%     \label{eq:net_aug_pred}
% \end{equation}
% The cleaned large-scale B-mode map, $T_L^{B, \text{pred}}(\hat{\mathbf{n}})$, is then obtained by subtracting this prediction from the observed large-scale B-mode map, $T_L^B(\hat{\mathbf{n}})$ (which itself consists of the large-scale CMB signal $S_L^B(\hat{\mathbf{n}})$ and the large-scale foregrounds $F_L^B(\hat{\mathbf{n}})$):
% \begin{equation}
%     T_L^{B, \text{pred}}(\hat{\mathbf{n}}) = T_L^B(\hat{\mathbf{n}}) - \hat{F}_L^B(\hat{\mathbf{n}}) = (S_L^B(\hat{\mathbf{n}}) + F_L^B(\hat{\mathbf{n}})) - \mathcal{N}_{\text{aug}}(F_S^B(\hat{\mathbf{n}}), I(\hat{\mathbf{n}}), E(\hat{\mathbf{n}})).
%     \label{eq:cleaned_map_aug}
% \end{equation}
% The structure of the foregrounds provides motivation for this augmentation. As detailed in Section \ref{sec:data}, both the `d9` and `d12` dust models generate small-scale fluctuations that are correlated with, or modulated by, large-scale features related to intensity ($I(\hat{\mathbf{n}})$) and polarization ($E(\hat{\mathbf{n}})$). For instance, in the `d9` model, the final small-scale Q and U maps (which determine B-modes) are constructed by modulating the original large-scale intensity map ($I_{L}$, related to T) with derived small-scale polarization properties (cf. discussion surrounding equations \ref{eq:d9_q_final} and \ref{eq:d9_u_final}). Similarly, in the `d12` model, the small-scale B-mode random field ($\delta B_{base}$) is explicitly modulated by the large-scale polarization amplitude ($P_{L}^i$, computed using the B- and E-modes) for each layer before harmonic combination (cf. Eqn.~\ref{eq:d12_mod_b}). 

% \section{Methods}\label{sec:methods}

% \subsection{Frequency-Difference UNet - Baseline}\label{sec:method_freqdiff}

% The frequency-difference UNet baseline follows \citet{mccarthy24_ml}:
% \begin{itemize}
%     \item \textbf{Input}: Observed CMB - ILC CMB (i.e., ILC foreground reconstructions), computed at multiple frequencies (95, 145, 220, 270 GHz).
%     \item \textbf{Target}: $\Delta$ILC (ILC residuals), defined as the difference between ILC-reconstructed CMB and true CMB.
% \end{itemize}

% The observed map at frequency $\nu_i$ can be written as:
% \be
% T_{\nu_i}(\hat{\mathbf{n}}) = S(\hat{\mathbf{n}}) + F_{\nu_i}(\hat{\mathbf{n}}) + N_{\nu_i}(\hat{\mathbf{n}}),
% \ee
% where $S(\hat{\mathbf{n}})$ is the CMB signal (frequency-independent), $F_{\nu_i}(\hat{\mathbf{n}})$ is the foreground at frequency $\nu_i$, and $N_{\nu_i}(\hat{\mathbf{n}})$ is instrumental noise. A frequency difference between channels $i$ and $j$ gives:
% \be
% \Delta T_{\nu_i,\nu_j}(\hat{\mathbf{n}}) = T_{\nu_i}(\hat{\mathbf{n}}) - T_{\nu_j}(\hat{\mathbf{n}}) = F_{\nu_i}(\hat{\mathbf{n}}) - F_{\nu_j}(\hat{\mathbf{n}}) + N_{\nu_i}(\hat{\mathbf{n}}) - N_{\nu_j}(\hat{\mathbf{n}}),
% \ee
% which contains no CMB signal by construction. 

% The network learns to predict ILC reconstruction errors from the frequency-dependent foreground characteristics present in the ILC foreground maps. Since both input and target are signal-free (CMB cancels in frequency differences and ILC residuals), the network preserves the cosmological signal by construction.

% \subsection{Interscale (Single-Frequency) UNet}\label{sec:method_interscale}

% We implement inter-scale cleaning using convolutional neural networks (U-Nets) \citep{ronneberger2015unet}, with two input configurations:

\section{Network Architecture}\label{sec:architecture}

We employ a U-Net architecture \citep{ronneberger2015unet}, which is well-suited for image-to-image translation tasks requiring multi-scale feature learning. The encoder-decoder structure with skip connections enables the network to capture correlations between small-scale input features (e.g., $\ell > 200$ foreground structures) and large-scale target features (e.g., $\ell < 200$ foreground contamination), while preserving fine-grained spatial details throughout the network depth.

The network adapts to different input configurations based on the training task:
\begin{itemize}
    \item \textbf{Single-frequency models}: 1 channel (B-mode only) or 3 channels (B + T + E modes) at a single frequency
    \item \textbf{Multi-frequency models}: 8 channels (4-frequency ILC foregrounds + 4-frequency small-scale B-modes) or 16 channels (8 channels above + 4-frequency E-modes + 4-frequency T-modes)
\end{itemize}
All configurations share the same core architecture: an encoder with feature dimensions $[32, 64, 128, 256, 512, 1024]$ using double convolutional blocks (3Ã—3 convolutions, batch normalization, LeakyReLU with negative slope $0.01$), average pooling for downsampling, and transpose convolutions with skip connections for upsampling. The single-channel output predicts either large-scale foregrounds (single-frequency models) or ILC residuals (multi-frequency models).

Complete architecture specifications, including layer details, hyperparameters, and training configurations, are provided in Appendix~\ref{app:nn_archi}.

\section{Training Configurations}\label{sec:training_configs}

We implement multiple network configurations to explore different strategies for foreground removal. Table~\ref{tab:configurations} summarizes all training configurations, organized by input type and number of channels.

% \begin{table}[h]
% \centering
% \caption{Summary of network training configurations. All configurations use U-Net architecture and are trained to predict foreground components or ILC residuals.}
% \label{tab:configurations}
% \small
% \begin{tabular}{p{2.5cm}p{1cm}p{6.5cm}p{3cm}}
% \toprule
% \textbf{Configuration} & \textbf{Ch.} & \textbf{Input Description} & \textbf{Target} \\
% \midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Inter-Scale:\\B-modes only\end{tabular}} & 1 & Small-scale B-modes ($\ell > 200$) at single frequency & \multirow{2}{*}{Large-scale B-mode foregrounds ($\ell < 200$)} \\
%  & & & \\
% \midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Inter-Scale:\\B + T + E\end{tabular}} & 3 & Small-scale B-modes ($\ell > 200$); Temperature (all scales); E-modes (all scales) & \multirow{2}{*}{Large-scale B-mode foregrounds ($\ell < 200$)} \\
%  & & & \\
% \midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Combined:\\Freq. + Scale\end{tabular}} & 8 & Ch. 0-3: ILC foregrounds at 95, 145, 220, 270 GHz; Ch. 4-7: Small-scale B-modes ($\ell > 200$) at 95, 145, 220, 270 GHz & \multirow{2}{*}{ILC residuals} \\
%  & & & \\
% \midrule
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Combined:\\Freq. + Scale\\+ T, E\end{tabular}} & 16 & Ch. 0-3: ILC foregrounds at 95, 145, 220, 270 GHz; Ch. 4-7: Small-scale B-modes ($\ell > 200$) at 95, 145, 220, 270 GHz; Ch. 8-11: All-scale E-modes at 95, 145, 220, 270 GHz; Ch. 12-15: All-scale T-modes at 95, 145, 220, 270 GHz & \multirow{4}{*}{ILC residuals} \\
%  & & & \\
%  & & & \\
%  & & & \\
% \bottomrule
% \end{tabular}
% \end{table}

The first two configurations (1 and 3 channels) test inter-scale learning using single-frequency maps decomposed by angular scale. These exploit correlations between small-scale and large-scale foreground structures while maintaining signal preservation, since primary CMB components at different scales are statistically independent.

The latter two configurations (8 and 16 channels) combine frequency-difference information (from ILC foreground reconstructions) with inter-scale information. This hybrid approach leverages both frequency-dependent foreground properties and scale-dependent correlations, potentially providing more robust foreground removal.
\subsection{Training Procedure}\label{sec:training}

Networks are trained to minimize the mean squared error (MSE) between predicted and true targets:
\be
L_{\text{MSE}} = \mathbb{E} \left\| \hat{Y} - Y \right\|^2,
\ee
where $\hat{Y}$ is the network prediction and $Y$ is the target (large-scale foregrounds for inter-scale models, ILC residuals for frequency-difference models).

Training uses the Adam optimizer with learning rate $10^{-4}$, batch size 32-64 (depending on model complexity), and early stopping based on validation loss. We employ data augmentation through random rotations and flips to increase dataset diversity. The training set consists of patches extracted from full-sky simulations, with train/validation/test splits of 80\%/10\%/10\%.

\section{Simulations}\label{sec:simulations}

\subsection{DustFilaments}\label{sec:dustfilaments}

We train and validate our models using simulated full-sky maps from the DustFilaments model \citep{Herv_as_Caimapo_2022}, which simulates Galactic thermal dust emission by populating the Galaxy with millions of individual filaments. The DustFilaments code\footnote{Available at \url{https://github.com/chervias/DustFilaments}} generates thermal dust maps by integrating a population of 3D filaments painted onto the celestial sphere, reproducing the statistical properties of \Planck 353 GHz dust polarization maps, including angular power spectra and non-Gaussian features, while providing independent realizations of the dust sky.

The model's three main components are: (1) \texttt{get\_MagField}, which generates a 3D magnetic field cube with an optional large-scale Galactic component and an isotropic random component; (2) \texttt{get\_FilPop}, which creates a random population of filaments aligned with the magnetic field; and (3) \texttt{Paint\_Filament}, which paints each filament into HEALPix maps of temperature and polarization (T, Q, U Stokes parameters) at specified frequencies. The filament population is generated deterministically given fixed seeds for both the magnetic field and the filament population, enabling reproducible realizations.

\subsubsection{Simulation Parameters}

We generated 150 independent full-sky realizations using the DustFilaments code with the following parameters:
\begin{itemize}
    \item \textbf{HEALPix resolution}: $N_{\text{side}} = 1024$ (pixel resolution $\sim 3.4$ arcmin, $12 N_{\text{side}}^2 = 12,582,912$ pixels)
    \item \textbf{Number of filaments}: 180,500,000 per realization
    \item \textbf{Frequencies}: 95, 145, 220, and 270 GHz (matching CMB-S4 and Simons Observatory frequency bands)
    \item \textbf{Maximum multipole}: $\ell_{\max} = 2000$ (harmonic decomposition limit for subsequent analysis)
    \item \textbf{Random seeds}: Each simulation uses a unique seed generated deterministically as $\text{seed} = 42424242 + n \times 104729$ for simulation index $n \in [1, 150]$, ensuring statistical independence between realizations while maintaining reproducibility
\end{itemize}

%Simulations were executed using MPI parallelization via SLURM job arrays, with each simulation running on 4 compute nodes using 16 MPI tasks per node and 8 OpenMP threads per task (64 total cores per simulation). Each full-sky realization required approximately 1--2 hours of wall-clock time, with the filament painting computation distributed across MPI processes such that each process handles $N_{\text{fils}}/N_{\text{processes}}$ filaments. The magnetic field cube and multi-resolution HEALPix maps (maintaining all resolutions from $N_{\text{side}} = 128$ up to the target resolution) were stored in memory per MPI process, as required by the DustFilaments algorithm.

\subsubsection{Output Maps and Processing}

For each simulation realization and frequency, the DustFilaments pipeline generates full-sky maps in HEALPix format containing:
\begin{itemize}
    \item \textbf{Foreground T, Q, U maps}: Full-sky Stokes parameter maps of thermal dust emission in thermodynamic temperature units ($\mu$K)
    \item \textbf{Foreground B-modes}: Computed from Q and U maps via spherical harmonic decomposition, yielding polarized dust emission $F^B(\hat{\mathbf{n}})$ at all scales
    \item \textbf{Foreground T and E modes}: Temperature $F^T(\hat{\mathbf{n}})$ and E-mode polarization $F^E(\hat{\mathbf{n}})$ derived from the T and Q/U maps respectively
\end{itemize}

We compute primordial CMB B-mode realizations independently using \textsc{CAMB} \citep{2011ascl.soft02026L} with \Planck 2018 cosmological parameters, yielding $S^B(\hat{\mathbf{n}})$ maps that are statistically independent from the foreground simulations. Observed maps are then constructed as $B(\hat{\mathbf{n}}) = S^B(\hat{\mathbf{n}}) + F^B(\hat{\mathbf{n}})$ at each frequency, combining the CMB signal with the dust foreground contamination.

\subsubsection{Patch Extraction and Scale Decomposition}

We extract $128 \times 128$ pixel patches (corresponding to $\sim 1.4^\circ \times 1.4^\circ$ at $N_{\text{side}} = 1024$) from full-sky maps using a Cylindrical Equal-Area (CAR) projection centered on randomly selected sky positions. Approximately 1,280 patches are extracted per full-sky simulation, yielding a total dataset of $\sim 192,000$ patches (150 simulations Ã— 1,280 patches per simulation).

Harmonic filtering is applied in spherical harmonic space to decompose foreground maps into large-scale ($\ell < 200$) and small-scale ($\ell > 200$) components:
\begin{itemize}
    \item \textbf{Large-scale foregrounds} ($F_L$): Target for single-frequency inter-scale models, representing contamination at the angular scales where primordial B-modes are expected ($\ell \lesssim 200$)
    \item \textbf{Small-scale foregrounds} ($F_S$): Input for inter-scale models, where foregrounds dominate the signal and the primordial CMB B-mode power is negligible ($\ell > 200$)
\end{itemize}

This scale separation exploits the statistical independence of CMB modes across different multipoles while leveraging the coherent structure of Galactic dust foregrounds across scales. For single-frequency models, CMB realizations are scaled by a factor of 0.05 to match expected signal amplitudes in observed data. The dataset is split into training (80\%), validation (10\%), and test (10\%) sets, with stratification by simulation index to ensure that patches from the same simulation do not appear in multiple splits, maintaining statistical independence between training, validation, and test data.

\section{Validation Metrics: Analyzing Model Performance, and Reconstructions}\label{sec:metrics}

We evaluate model performance using the following validation metrics:

\subsection{MSE}
For all networks, we compute multiple types of Mean Square Error (MSE) metrics. For multi-frequency models predicting ILC residuals, we compute the prediction MSE for each patch:
\begin{equation}
    \text{MSE}_{\text{pred}} = \frac{1}{N_p} \sum_{p=1}^{N_p} \left(\Delta\hat{T}^{\mathrm{ILC}}(\hat{\mathbf{n}}_p) - \Delta T^{\mathrm{ILC}}(\hat{\mathbf{n}}_p)\right)^2,
    \label{eq:pred_mse_metric}
\end{equation}
where $\Delta\hat{T}^{\mathrm{ILC}}(\hat{\mathbf{n}}_p)$ is the network-predicted ILC residual, and $\Delta T^{\mathrm{ILC}}(\hat{\mathbf{n}}_p)$ is the true ILC residual. This metric quantifies how accurately the network predicts the ILC residual target.

For single-frequency models predicting large-scale foregrounds, we compute the foreground prediction MSE:
\begin{equation}
    \text{MSE}_{\text{pred}} = \frac{1}{N_p} \sum_{p=1}^{N_p} \left(\hat{F}_L(\hat{\mathbf{n}}_p) - F_L(\hat{\mathbf{n}}_p)\right)^2,
\end{equation}
where $\hat{F}_L$ is the predicted large-scale foreground and $F_L$ is the true large-scale foreground.

We also compute the CMB reconstruction MSE for both single-frequency and multi-frequency models:
\begin{equation}
    \text{MSE}_{\text{recon}} = \frac{1}{N_p} \sum_{p=1}^{N_p} \left(\hat{S}^{\mathrm{coi}}(\hat{\mathbf{n}}_p) - S^{\mathrm{coi}}(\hat{\mathbf{n}}_p)\right)^2,
    \label{eq:recon_mse_metric}
\end{equation}
where $\hat{S}^{\mathrm{coi}}$ is the final cleaned CMB reconstruction and $S^{\mathrm{coi}}$ is the true CMB signal. This MSE quantifies the improvement the network achieves in the final primary CMB reconstruction compared to the baseline ILC or observed maps. We compare $\text{MSE}_{\text{recon}}$ of the network-corrected reconstruction with that of the baseline method (ILC for multi-frequency models, observed maps for single-frequency models). A lower MSE indicates improved cleaning performance.

Additionally, we compute the variance of the CMB reconstructions ($\text{Var}[\hat{S}^{\mathrm{coi}}]$) and compare it to the true CMB variance ($\text{Var}[S^{\mathrm{coi}}]$) to assess whether the reconstruction preserves the signal power spectrum appropriately.

\subsection{Spatial Correlations}
We evaluate the spatial alignment between predicted and true maps using the Pearson correlation coefficient. For foreground predictions, we compute:
\begin{equation}
    \rho_{\text{spatial}} = \frac{\text{Cov}(\hat{F}_L, F_L)}{\sigma_{\hat{F}_L} \sigma_{F_L}},
\end{equation}
where $\text{Cov}$ denotes covariance and $\sigma$ represents standard deviation. For CMB reconstructions, we compute the correlation between the reconstructed and true CMB maps:
\begin{equation}
    \rho_{\text{spatial}} = \frac{\text{Cov}(\hat{S}^{\mathrm{coi}}, S^{\mathrm{coi}})}{\sigma_{\hat{S}^{\mathrm{coi}}} \sigma_{S^{\mathrm{coi}}}}.
\end{equation}

To assess the statistical significance of these correlations, we perform null hypothesis tests by computing the distribution of correlation coefficients between each predicted/reconstructed map and 100 randomly selected target/CMB patches from other test samples (excluding the matching patch). The actual correlation is compared against this null distribution, with correlations significantly exceeding the null distribution (typically $>5\sigma$ above the mean) confirming that the network has genuinely learned predictive spatial patterns rather than correlating by chance.

\subsection{Normalized Cross-Power Spectra}

We evaluate correlations in harmonic space through normalized cross-power spectra. For foreground predictions, we compute the cross-power spectrum:
\begin{equation}
    C_\ell^{\hat{F}_L \times F_L} = \frac{1}{2\ell+1} \sum_{m=-\ell}^{\ell} \hat{a}_{\ell m}^{\hat{F}_L} (a_{\ell m}^{F_L})^*,
\end{equation}
where $\hat{a}_{\ell m}^{\hat{F}_L}$ and $a_{\ell m}^{F_L}$ are the spherical harmonic coefficients of the predicted and true foreground maps, respectively. The cross-power spectrum is normalized by the geometric mean of the auto-power spectra:
\begin{equation}
    \tilde{C}_\ell^{\hat{F}_L \times F_L} = \frac{C_\ell^{\hat{F}_L \times F_L}}{\sqrt{C_\ell^{\hat{F}_L \times \hat{F}_L} C_\ell^{F_L \times F_L}}},
\end{equation}
yielding a normalized cross-power spectrum that ranges between $-1$ and $+1$. The resulting spectrum is binned with bin width $\Delta \ell = 50$ and computed separately for all-scale ($\ell \leq 2000$) and large-scale ($\ell < 200$) analyses for foreground reconstructions.

For CMB reconstructions, we compute the normalized cross-power spectrum between the reconstructed and true CMB maps:
\begin{equation}
    \tilde{C}_\ell^{\hat{S} \times S} = \frac{C_\ell^{\hat{S} \times S}}{\sqrt{C_\ell^{\hat{S} \times \hat{S}} C_\ell^{S \times S}}}.
\end{equation}
This normalized cross-power spectrum is used to verify the unbiasedness condition given in Equation~\eqref{eq:unbiasedness_condition}. Specifically, we verify that $\tilde{C}_\ell^{\hat{S} \times S}$ approaches unity across all multipoles ($\ell < 200$ for large-scale analysis), which demonstrates that the reconstruction preserves the cosmological signal without bias. This validation is critical for ensuring that the signal-preserving framework is functioning correctly and that subsequent cosmological inference remains unbiased.

To assess statistical significance, we compute null cross-power spectra by calculating the cross-power spectrum between each reconstructed map and random CMB/foreground patches from other test samples. We report the mean cross-power spectrum across all patches, along with the 16th and 84th percentiles (equivalent to $\pm 1\sigma$ for a Gaussian distribution) of the distribution across patches at each multipole bin, providing uncertainty bands that reflect the patch-to-patch variation in reconstruction quality.

\section{Results}\label{sec:results}

\subsection{Single-Frequency Inputs}\label{sec:results_interscale}

\subsubsection{Small-Scale $\rightarrow$ Large-Scale B-modes}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/singlefreq_B-only.png}
    \caption{Foreground prediction quality for a representative test patch. Top row: (a) Small-scale foreground B-modes ($\ell > 200$, input), (b) Large-scale foreground B-modes ($\ell < 200$, target), (c) UNet prediction. Bottom row: (d) Normalized cross-power spectrum between prediction and target with mean $\pm 1\sigma$ uncertainty bands and null cross-spectrum, (e) Null correlation test histogram with actual correlation (green solid line) and mean correlation (blue dashed line) marked, (f) MSE comparison showing prediction error vs. target power (hexbin density plot) with diagonal reference line.}
    \label{fig:b_only_fg_recon}
\end{figure}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\textwidth]{Figures/fg_reconstruction_sample_104134.png}
%     \caption{Foreground prediction quality for a representative test patch. Top row: (a) Small-scale foreground B-modes ($\ell > 200$, input), (b) Large-scale foreground B-modes ($\ell < 200$, target), (c) UNet prediction. Bottom row: (d) Normalized cross-power spectrum between prediction and target with mean $\pm 1\sigma$ uncertainty bands and null cross-spectrum, (e) Null correlation test histogram with actual correlation (green solid line) and mean correlation (blue dashed line) marked, (f) MSE comparison showing prediction error vs. target power (hexbin density plot) with diagonal reference line.}
%     \label{fig:fg_reconstruction}
% \end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/cmb_reconstruction_sample_104134.png}
    \caption{CMB reconstruction quality for a representative test patch. Left to right: (a) Pure primordial CMB B-modes (ground truth), (b) Observed B all-scales (uncleaned), (c) UNet CMB reconstruction with spatial correlation coefficients comparing reconstructed CMB to pure CMB for both observed (uncleaned) and UNet-cleaned cases, (d) Normalized cross-power spectra comparing UNet and observed reconstructions to pure CMB with mean $\pm 1\sigma$ uncertainty bands and null cross-spectrum, (e) Null correlation test histogram with actual correlation marked (green dashed line) and mean of null distribution (gray vertical line), (f) MSE comparison across all test patches (hexbin density plot) with diagonal reference line and sample position marked.}
    \label{fig:cmb_reconstruction}
\end{figure}

We evaluate the network's ability to reconstruct large-scale foreground B-modes from small-scale information. Figure~\ref{fig:b_only_fg_recon} shows representative examples of foreground reconstruction for test patches. The top row displays spatial maps: (a) small-scale foreground B-modes ($\ell > 200$, input channel), (b) large-scale foreground B-modes ($\ell < 200$, target), and (c) UNet prediction. The bottom row shows quantitative metrics: (d) normalized cross-power spectrum between prediction and target, (e) null correlation test histogram, and (f) MSE comparison hexbin plot.

\textbf{Spatial Correlation Analysis}: Across the test set, we achieve a mean spatial correlation of $0.45 \pm 0.29$ (mean $\pm$ standard deviation) between predicted and true large-scale foregrounds, with $35\%$ of patches showing correlations above $0.5$. The null correlation test (panel e) compares the actual correlation for each patch against the distribution of correlations between the prediction and 100 randomly selected target patches. For representative patches, the actual correlation lies $>5\sigma$ above the mean of the null distribution, confirming statistical significance.

\textbf{Harmonic Space Correlation}: The mean cross-spectrum correlation across all test patches is $0.49$, indicating strong harmonic-space fidelity. Panel (d) shows the cross-power spectrum for a representative patch (green solid line), compared to the mean $\pm 1\sigma$ uncertainty band across all patches (brown dashed line with shaded region) and the null cross-spectrum (gray dotted line).

\textbf{Mean Squared Error}: The mean MSE between predicted and true large-scale foregrounds is $3.5 \times 10^{-4}$ on the test set, compared to the inherent power of the target (MSE between target and zero) of $\sim 10^{-2}$.

\subsubsection{Impact of Temperature and E-Modes}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/singlefreq_TEB.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

Figure~\ref{fig:mse_progression} shows the MSE progression from baseline to multi-channel models. The B-mode only model achieves significant improvement over the baseline, and the T,E,B model further reduces MSE, demonstrating the benefit of incorporating multi-channel information.

Figure~\ref{fig:correlation_comparison} compares the distribution of spatial correlations between UNet predictions and true targets for the two model configurations. Mean test correlations are $0.45 \pm 0.29$ (B-only) and $0.48 \pm 0.28$ (T,E,B), with the multi-channel model showing slightly better average performance and more consistent results (smaller standard deviation).

Figure~\ref{fig:input_channels} (generated when using T+E channels) illustrates the three-channel input to the UNet: (a) small-scale foreground B-modes ($\ell > 200$), (b) foreground temperature (all scales), and (c) foreground E-modes (all scales).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/cross_spectrum_comparison.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{CMB Reconstruction Quality}

We evaluate the downstream impact on CMB reconstruction by computing cleaned maps: $B^{\text{pred}}_L = B_L - \hat{F}_L$. Figure~\ref{fig:cmb_reconstruction} shows a comprehensive 1$\times$6 panel analysis for representative test patches.

\textbf{Spatial Correlation with True CMB}: The mean spatial correlation between UNet-cleaned CMB reconstructions and pure CMB is $0.82 \pm 0.15$ on the test set, compared to $0.45 \pm 0.20$ for uncleaned observed B-modes.

\textbf{Cross-Power Spectra}: The UNet reconstruction shows significantly higher cross-power than the uncleaned observed signal across all multipoles, with the cross-spectrum approaching unity at large scales ($\ell < 100$). The cross-spectrum between UNet reconstruction and true CMB matches the true CMB auto-spectrum, confirming signal preservation.

\textbf{MSE Comparison}: On average, the UNet reconstruction achieves a $60\%$ reduction in MSE compared to uncleaned observations.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/single_freq_compare_corr.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsection{Frequency-Difference + Inter-Scale UNet}\label{sec:results_combined}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/compare_all_cross_spectra.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}
[Results for 8-channel model combining ILC foregrounds and small-scale B-modes across 4 frequencies]
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/rows_single_freq.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{16 Channels: Frequency Difference + Small-Scale B + T, E}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Figures/compare-all.png}
    \caption{Similar to Fig. 12 in \cite{mccarthy24_ml} paper, showing the ILC residuals (target) and the UNet's predicted residuals. \textbf{Top row:} Results when using frequency-differences, small-scale b modes, and T, E maps as inputs. \textbf{Middle row:} Results when using both frequency-differences and small-scale b modes. The Unet is able to remove a lot of the foreground variance that the ILC solution still contained, reducing the residual by over 90\% \textbf{Bottom row:} Results when using just frequency-differences. The residuals is slightly higher, by ~30\%, compared to the multi-freq+inter-scale Unet above.
}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/sample_18.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

[Results for 16-channel model adding T and E modes across 4 frequencies]

\subsection{ILC with Additional Channel}\label{sec:results_ilc_enhanced}

We evaluate the effectiveness of using UNet predictions as an additional channel in the ILC combination. The UNet-enhanced ILC uses the UNet-predicted CMB reconstruction as a synthetic frequency channel, allowing the ILC to optimally combine multi-frequency observations with the ML-enhanced prediction.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/ilc_enhanced_teb.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/ilc_enhanced_b.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}
[Results comparing vanilla ILC (4 frequencies) vs. enhanced ILC (4 frequencies + UNet channel)]

\section{Discussion}\label{sec:discussion}

[Discussion of key findings, comparison with previous work, limitations, and future directions]

\section{Conclusions}\label{sec:conclusions}

[Summary of main results and implications]

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/input_channels_sample_4321.png}
    \caption{UNet input channels for a representative test patch (generated when using T+E channels): (a) Small-scale B-modes ($\ell > 200$), (b) Temperature (all scales), (c) E-modes (all scales). These panels illustrate the three-channel input to the UNet, showing the spatial structure of the information available to the model for predicting large-scale B-mode foregrounds.}
    \label{fig:input_channels}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/mse_progression.png}
    \caption{MSE progression showing systematic improvement from ILC baseline (single-frequency, no prediction) to B-mode only UNet to T,E,B multi-channel UNet. Error bars represent $\pm 1$ standard deviation across all test patches. Y-axis uses logarithmic scaling. Connecting lines emphasize the decreasing trend. Numerical MSE values are displayed above each point for precise quantification. \helen{Add the multif-freq-interscale unet MSE's to this plot}}
    \label{fig:mse_progression}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/correlation_comparison.png}
    \caption{Spatial correlation distributions for B-mode only (teal) and T,E,B multi-channel (orange) UNet models. Distributions are approximated from mean and standard deviation statistics using Gaussian distributions, shown as semi-transparent filled areas. Vertical dashed lines mark the mean correlation for each model. Higher mean correlation with smaller standard deviation indicates better and more consistent performance.}
    \label{fig:correlation_comparison}
\end{figure}

\appendix
\section{Network Architecture and Training Details}\label{app:nn_archi}

\subsection{Architecture Specification}\label{app:arch_spec}

The U-Net architecture follows the encoder-decoder structure of \citet{ronneberger2015unet} with the following detailed specifications:

\subsubsection{Network Depth and Feature Dimensions}
The network consists of 6 encoder levels and 6 decoder levels, with feature channel dimensions $[32, 64, 128, 256, 512, 1024]$. The encoder progressively down-samples the input from $128 \times 128$ to $4 \times 4$ pixels (bottleneck), while the decoder up-samples back to $128 \times 128$ pixels using transpose convolutions and skip connections.

\subsubsection{DoubleConv Block Architecture}
Each encoder and decoder block contains a \texttt{DoubleConv} module consisting of two sequential $3 \times 3$ convolutions with the following configuration:
\begin{itemize}
    \item Convolution parameters: kernel size $3 \times 3$, stride $1$, padding $1$ (maintains spatial dimensions)
    \item Normalization: Batch normalization after each convolution
    \item Activation: LeakyReLU with negative slope $\alpha = 0.01$
    \item Bias: Disabled (BatchNorm provides learnable shift)
\end{itemize}

\subsubsection{Downsampling and Upsampling}
\begin{itemize}
    \item \textbf{Encoder downsampling}: Average pooling with $2 \times 2$ kernel and stride $2$ between encoder levels
    \item \textbf{Decoder upsampling}: Transpose convolution with $2 \times 2$ kernel and stride $2$ between decoder levels
    \item \textbf{Skip connections}: Encoder feature maps are concatenated channel-wise with upsampled decoder features before each decoder \texttt{DoubleConv} block
    \item \textbf{Spatial alignment}: Bilinear interpolation handles size mismatches when concatenating skip connections (for odd spatial dimensions)
\end{itemize}

\subsubsection{Bottleneck}
The bottleneck layer expands the deepest feature representation from 1024 to 2048 channels using a \texttt{DoubleConv} block at $4 \times 4$ spatial resolution.

\subsubsection{Output Layer}
The final layer is a $1 \times 1$ convolution mapping from 32 channels to the output channel count (1 channel for all models). For unnormalized training, the output layer bias is initialized to the training set mean of the target variable to provide a sensible starting point.

\subsubsection{Parameter Count and Memory}
Total trainable parameters vary with input channel count:
\begin{itemize}
    \item Single-frequency (3 channels): $\sim 15$ million parameters
    \item Multi-frequency (8 channels): $\sim 16$ million parameters
    \item Multi-frequency (16 channels): $\sim 18$ million parameters
\end{itemize}
Training requires $\sim 8$--12 GB GPU memory (batch size 32--64), while inference requires $\sim 2$ GB GPU memory.

\subsection{Training Configuration}\label{app:training_config}

\subsubsection{Loss Function and Optimization}
Networks are trained to minimize the Mean Squared Error (MSE) loss:
\begin{equation}
    L = \frac{1}{N} \sum_{i=1}^{N} \| \hat{Y}_i - Y_i \|^2,
\end{equation}
where $\hat{Y}_i$ is the network prediction and $Y_i$ is the target (large-scale foregrounds for single-frequency models, ILC residuals for multi-frequency models).

Optimization uses the Adam optimizer \citep{kingma2014adam} with the following hyperparameters:
\begin{itemize}
    \item \textbf{Learning rate}: $10^{-3}$ for single-frequency models, $10^{-4}$ for multi-frequency models (initial)
    \item \textbf{Weight decay}: $5 \times 10^{-4}$ (single-frequency), $1 \times 10^{-5}$ (multi-frequency)
    \item \textbf{Adam parameters}: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$
\end{itemize}

\subsubsection{Learning Rate Schedule}
Training employs a two-stage learning rate schedule:
\begin{itemize}
    \item \textbf{Warmup phase}: Linear warmup over 5 epochs (single-frequency) or 10 epochs (multi-frequency), starting from $10^{-5}$ or $10^{-6}$ respectively
    \item \textbf{Adaptive reduction}: \texttt{ReduceLROnPlateau} scheduler reduces learning rate by factor 0.5 when validation loss plateaus (patience = 10 epochs), with minimum learning rate $10^{-6}$
    \item \textbf{Dataset size scaling}: Multi-frequency models scale initial learning rate by $\sqrt{N_{\text{data}} / 100}$ to account for dataset size variations
\end{itemize}

\subsubsection{Training Procedure}
\begin{itemize}
    \item \textbf{Batch size}: 32 (single-frequency), 64 (multi-frequency)
    \item \textbf{Maximum epochs}: 200
    \item \textbf{Early stopping}: Training stops if validation loss does not improve for 20 epochs (single-frequency) or based on validation performance (multi-frequency). Best model (lowest validation loss) is saved.
    \item \textbf{Gradient clipping}: Maximum gradient norm clipped to 1.0 (single-frequency) or 5.0 (multi-frequency) to stabilize training
\end{itemize}

\subsubsection{Data Augmentation}
Training data augmentation includes spatial transformations applied randomly with 50\% probability:
\begin{itemize}
    \item Random rotations: $90^\circ$, $180^\circ$, or $270^\circ$
    \item Random flips: Horizontal and/or vertical axis
\end{itemize}
Augmentation is applied only to training data; validation and test sets remain unaugmented.

\subsubsection{Data Normalization}
Two normalization strategies are employed:
\begin{itemize}
    \item \textbf{Normalized training}: Per-channel standardization using training set mean and standard deviation for inputs and targets separately
    \item \textbf{Unnormalized training}: No normalization, with output layer bias initialized to training set mean (used for some multi-frequency models)
\end{itemize}

\subsubsection{Initialization}
\begin{itemize}
    \item \textbf{Convolutional layers}: Kaiming normal initialization \citep{he2015delving}
    \item \textbf{Batch normalization}: Weight initialized to 1, bias to 0
    \item \textbf{Final layer}: Xavier normal for normalized training; weights = 0, bias = training set mean for unnormalized training
\end{itemize}

\subsubsection{Hardware and Computational Requirements}
Training was performed on NVIDIA A100 or V100 GPUs using PyTorch with float32 precision. Training times vary by model:
\begin{itemize}
    \item Single-frequency models: $\sim 2$--4 hours for 200 epochs
    \item Multi-frequency models: $\sim 4$--8 hours for 200 epochs
\end{itemize}

\section*{Acknowledgments}

\bibliographystyle{apsrev4-2}
\bibliography{references}

\end{document}
